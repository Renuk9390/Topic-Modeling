{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grant_id</th>\n",
       "      <th>claims_text</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US10459019</td>\n",
       "      <td>1. An electromagnetic sensor comprising:,a fir...</td>\n",
       "      <td>An electromagnetic sensor includes a first mag...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US10456083</td>\n",
       "      <td>1. A method for mapping somatosensory and moto...</td>\n",
       "      <td>An apparatus for cortical mapping and method f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US10461549</td>\n",
       "      <td>1. A method for charging a mobile terminal, th...</td>\n",
       "      <td>The disclosure discloses a mobile terminal, a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US10462815</td>\n",
       "      <td>1. A method for a User Equipment (UE) operatin...</td>\n",
       "      <td>The present invention relates to a wireless co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US10458026</td>\n",
       "      <td>1. A method of producing graphene sheets compr...</td>\n",
       "      <td>A method of producing graphene sheets comprisi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>US10458777</td>\n",
       "      <td>1. A method of measuring a metrology target el...</td>\n",
       "      <td>Targets, target elements and target design met...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>US10458022</td>\n",
       "      <td>1. A method for anti-corrosive treatment of me...</td>\n",
       "      <td>A method for corrosion protection treatment, c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>US10462550</td>\n",
       "      <td>1. A storage device comprising:,a first case c...</td>\n",
       "      <td>A storage device includes a first case, a seco...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400</th>\n",
       "      <td>US10456037</td>\n",
       "      <td>1. A terminal device configured to be able to ...</td>\n",
       "      <td>A terminal device is provided which is configu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1401</th>\n",
       "      <td>US10461138</td>\n",
       "      <td>1. An organic light-emitting display device, c...</td>\n",
       "      <td>An organic light-emitting display device and a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1402 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        grant_id                                        claims_text  \\\n",
       "0     US10459019  1. An electromagnetic sensor comprising:,a fir...   \n",
       "1     US10456083  1. A method for mapping somatosensory and moto...   \n",
       "2     US10461549  1. A method for charging a mobile terminal, th...   \n",
       "3     US10462815  1. A method for a User Equipment (UE) operatin...   \n",
       "4     US10458026  1. A method of producing graphene sheets compr...   \n",
       "...          ...                                                ...   \n",
       "1397  US10458777  1. A method of measuring a metrology target el...   \n",
       "1398  US10458022  1. A method for anti-corrosive treatment of me...   \n",
       "1399  US10462550  1. A storage device comprising:,a first case c...   \n",
       "1400  US10456037  1. A terminal device configured to be able to ...   \n",
       "1401  US10461138  1. An organic light-emitting display device, c...   \n",
       "\n",
       "                                               abstract  \n",
       "0     An electromagnetic sensor includes a first mag...  \n",
       "1     An apparatus for cortical mapping and method f...  \n",
       "2     The disclosure discloses a mobile terminal, a ...  \n",
       "3     The present invention relates to a wireless co...  \n",
       "4     A method of producing graphene sheets comprisi...  \n",
       "...                                                 ...  \n",
       "1397  Targets, target elements and target design met...  \n",
       "1398  A method for corrosion protection treatment, c...  \n",
       "1399  A storage device includes a first case, a seco...  \n",
       "1400  A terminal device is provided which is configu...  \n",
       "1401  An organic light-emitting display device and a...  \n",
       "\n",
       "[1402 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read uspto dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "usptodata = pd.read_csv('U.S. Patents.csv')\n",
    "usptodataset=usptodata[[\"grant_id\",\"claims_text\",\"abstract\"]]\n",
    "usptodataset= usptodataset.dropna()\n",
    "usptodataset = usptodataset.reset_index(drop=True)\n",
    "\n",
    "#split data to have little data to run\n",
    "# divide dataset to train and test\n",
    "df_train, df_test = train_test_split(usptodataset, test_size=0.8, random_state=25)\n",
    "df_train=df_train.reset_index(drop=True)\n",
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.8/site-packages/pandas/core/dtypes/inference.py:181: FutureWarning: Possible nested set at position 1\n",
      "  re.compile(obj)\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "import string\n",
    "\n",
    "# NLTK English stopwords\n",
    "new_stopwords = [\"shown\", \"design\", \"ornamental\", \"describe\", \"described\", \"described.\"]\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend(new_stopwords)\n",
    "\n",
    "\n",
    "def cleantext(df): \n",
    "    \n",
    "    df_train['cleaned_text'] = df_train['claims_text'].replace(r'\\'|\\\"|\\,|\\.|\\?|\\+|\\-|\\/|\\=|\\(|\\)|\\n|\"', '', regex=True)\n",
    "    df_train['cleaned_text'] = df_train['cleaned_text'].replace(\"  \", \" \")\n",
    "    \n",
    "    # convert tweets to lowercase\n",
    "    df_train['cleaned_text'] = df_train['cleaned_text'].str.lower()\n",
    "    \n",
    "    #remove numbers\n",
    "    df_train['cleaned_text'] =df_train['cleaned_text'].replace(r'\\d+', '', regex = True)\n",
    "        \n",
    "    #remove_symbols\n",
    "    df_train['cleaned_text']  = df_train['cleaned_text'].replace(r'[^a-zA-Z0-9]', \" \", regex=True)\n",
    "    \n",
    "    #remove punctuations \n",
    "    df_train['cleaned_text'] = df_train['cleaned_text'].replace(r'[[]!\"#$%\\'()\\*+,-./:;<=>?^_`{|}]+',\"\", regex = True)\n",
    "    \n",
    "    #remove_URL(x):\n",
    "    df_train['fully_cleaned_text']  = df_train['cleaned_text'].replace(r'https.*$', \"\", regex = True)\n",
    "    \n",
    "    #remove stopwords and words_to_remove\n",
    "    #df_train['fully_cleaned_text'] = df_train['cleaned_text'].apply(lambda x: ' '.join([word for word in str(x).split() if word not in stopwords]))\n",
    "    \n",
    "    \n",
    "    return df_train\n",
    "\n",
    "all_texts = cleantext(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grant_id</th>\n",
       "      <th>claims_text</th>\n",
       "      <th>abstract</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>fully_cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>US10459019</td>\n",
       "      <td>1. An electromagnetic sensor comprising:,a fir...</td>\n",
       "      <td>An electromagnetic sensor includes a first mag...</td>\n",
       "      <td>an electromagnetic sensor comprising a first ...</td>\n",
       "      <td>an electromagnetic sensor comprising a first ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>US10456083</td>\n",
       "      <td>1. A method for mapping somatosensory and moto...</td>\n",
       "      <td>An apparatus for cortical mapping and method f...</td>\n",
       "      <td>a method for mapping somatosensory and motor ...</td>\n",
       "      <td>a method for mapping somatosensory and motor ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US10461549</td>\n",
       "      <td>1. A method for charging a mobile terminal, th...</td>\n",
       "      <td>The disclosure discloses a mobile terminal, a ...</td>\n",
       "      <td>a method for charging a mobile terminal the m...</td>\n",
       "      <td>a method for charging a mobile terminal the m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>US10462815</td>\n",
       "      <td>1. A method for a User Equipment (UE) operatin...</td>\n",
       "      <td>The present invention relates to a wireless co...</td>\n",
       "      <td>a method for a user equipment ue operating in...</td>\n",
       "      <td>a method for a user equipment ue operating in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>US10458026</td>\n",
       "      <td>1. A method of producing graphene sheets compr...</td>\n",
       "      <td>A method of producing graphene sheets comprisi...</td>\n",
       "      <td>a method of producing graphene sheets compris...</td>\n",
       "      <td>a method of producing graphene sheets compris...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1397</th>\n",
       "      <td>US10458777</td>\n",
       "      <td>1. A method of measuring a metrology target el...</td>\n",
       "      <td>Targets, target elements and target design met...</td>\n",
       "      <td>a method of measuring a metrology target elem...</td>\n",
       "      <td>a method of measuring a metrology target elem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1398</th>\n",
       "      <td>US10458022</td>\n",
       "      <td>1. A method for anti-corrosive treatment of me...</td>\n",
       "      <td>A method for corrosion protection treatment, c...</td>\n",
       "      <td>a method for anticorrosive treatment of metal...</td>\n",
       "      <td>a method for anticorrosive treatment of metal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1399</th>\n",
       "      <td>US10462550</td>\n",
       "      <td>1. A storage device comprising:,a first case c...</td>\n",
       "      <td>A storage device includes a first case, a seco...</td>\n",
       "      <td>a storage device comprising a first case comp...</td>\n",
       "      <td>a storage device comprising a first case comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1400</th>\n",
       "      <td>US10456037</td>\n",
       "      <td>1. A terminal device configured to be able to ...</td>\n",
       "      <td>A terminal device is provided which is configu...</td>\n",
       "      <td>a terminal device configured to be able to co...</td>\n",
       "      <td>a terminal device configured to be able to co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1401</th>\n",
       "      <td>US10461138</td>\n",
       "      <td>1. An organic light-emitting display device, c...</td>\n",
       "      <td>An organic light-emitting display device and a...</td>\n",
       "      <td>an organic lightemitting display device compr...</td>\n",
       "      <td>an organic lightemitting display device compr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1402 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        grant_id                                        claims_text  \\\n",
       "0     US10459019  1. An electromagnetic sensor comprising:,a fir...   \n",
       "1     US10456083  1. A method for mapping somatosensory and moto...   \n",
       "2     US10461549  1. A method for charging a mobile terminal, th...   \n",
       "3     US10462815  1. A method for a User Equipment (UE) operatin...   \n",
       "4     US10458026  1. A method of producing graphene sheets compr...   \n",
       "...          ...                                                ...   \n",
       "1397  US10458777  1. A method of measuring a metrology target el...   \n",
       "1398  US10458022  1. A method for anti-corrosive treatment of me...   \n",
       "1399  US10462550  1. A storage device comprising:,a first case c...   \n",
       "1400  US10456037  1. A terminal device configured to be able to ...   \n",
       "1401  US10461138  1. An organic light-emitting display device, c...   \n",
       "\n",
       "                                               abstract  \\\n",
       "0     An electromagnetic sensor includes a first mag...   \n",
       "1     An apparatus for cortical mapping and method f...   \n",
       "2     The disclosure discloses a mobile terminal, a ...   \n",
       "3     The present invention relates to a wireless co...   \n",
       "4     A method of producing graphene sheets comprisi...   \n",
       "...                                                 ...   \n",
       "1397  Targets, target elements and target design met...   \n",
       "1398  A method for corrosion protection treatment, c...   \n",
       "1399  A storage device includes a first case, a seco...   \n",
       "1400  A terminal device is provided which is configu...   \n",
       "1401  An organic light-emitting display device and a...   \n",
       "\n",
       "                                           cleaned_text  \\\n",
       "0      an electromagnetic sensor comprising a first ...   \n",
       "1      a method for mapping somatosensory and motor ...   \n",
       "2      a method for charging a mobile terminal the m...   \n",
       "3      a method for a user equipment ue operating in...   \n",
       "4      a method of producing graphene sheets compris...   \n",
       "...                                                 ...   \n",
       "1397   a method of measuring a metrology target elem...   \n",
       "1398   a method for anticorrosive treatment of metal...   \n",
       "1399   a storage device comprising a first case comp...   \n",
       "1400   a terminal device configured to be able to co...   \n",
       "1401   an organic lightemitting display device compr...   \n",
       "\n",
       "                                     fully_cleaned_text  \n",
       "0      an electromagnetic sensor comprising a first ...  \n",
       "1      a method for mapping somatosensory and motor ...  \n",
       "2      a method for charging a mobile terminal the m...  \n",
       "3      a method for a user equipment ue operating in...  \n",
       "4      a method of producing graphene sheets compris...  \n",
       "...                                                 ...  \n",
       "1397   a method of measuring a metrology target elem...  \n",
       "1398   a method for anticorrosive treatment of metal...  \n",
       "1399   a storage device comprising a first case comp...  \n",
       "1400   a terminal device configured to be able to co...  \n",
       "1401   an organic lightemitting display device compr...  \n",
       "\n",
       "[1402 rows x 5 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "\n",
    "def get_bigrams(myString):\n",
    "    tokenizer = WordPunctTokenizer()\n",
    "    tokens = tokenizer.tokenize(myString)\n",
    "    stemmer = PorterStemmer()\n",
    "    bigram_finder = BigramCollocationFinder.from_words(tokens)\n",
    "    bigrams = bigram_finder.nbest(BigramAssocMeasures.chi_sq, 500)\n",
    "\n",
    "    for bigram_tuple in bigrams:\n",
    "        x = \"%s %s\" % bigram_tuple\n",
    "        tokens.append(x)\n",
    "\n",
    "    result = [' '.join([stemmer.stem(w).lower() for w in x.split()]) for x in tokens if x.lower() not in stopwords.words('english') and len(x) > 8]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in all_texts['fully_cleaned_text']:\n",
    "    bigrams = get_bigrams(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up(ngrams):\n",
    "    seen = set()\n",
    "    for ngram in ngrams:\n",
    "        if ' ' in ngram:\n",
    "            seen = seen.union(set(ngram.split()))\n",
    "    return [ngram for ngram in ngrams if ngram not in seen]\n",
    "\n",
    "\n",
    "bigrams = clean_up(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "n_wd_bigrams = np.empty((len(bigrams), len(all_texts['fully_cleaned_text'])))\n",
    "\n",
    "for i in range(len(bigrams)):\n",
    "    for j in range(len(all_texts['fully_cleaned_text'])):\n",
    "        n_wd_bigrams[i][j] = all_texts['fully_cleaned_text'][j].count(bigrams[i])\n",
    "        \n",
    "cv = CountVectorizer(max_features = len(bigrams), stop_words='english')\n",
    "n_wd = np.array(cv.fit_transform(all_texts['fully_cleaned_text']).todense()).T\n",
    "vocabulary = cv.get_feature_names_out()\n",
    "\n",
    "n_wd = np.concatenate((n_wd, n_wd_bigrams))\n",
    "vocabulary += bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.8/site-packages/artm/batches_utils.py:227: DeprecationWarning: Please use `spmatrix` from the `scipy.sparse` namespace, the `scipy.sparse.base` namespace is deprecated.\n",
      "  from scipy.sparse.base import spmatrix\n"
     ]
    }
   ],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "bv = artm.BatchVectorizer(data_format='bow_n_wd',\n",
    "                          n_wd=n_wd[:len(vocabulary)],\n",
    "                          vocabulary=vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = artm.LDA(num_topics=15, dictionary=bv.dictionary)\n",
    "#model.fit_offline(bv, num_collection_passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import artm\n",
    "topic_names = ['topic_{}'.format(i) for i in range(10)]\n",
    "\n",
    "model_artm = artm.ARTM(topic_names = topic_names, cache_theta=True, scores=[artm.PerplexityScore(name='PerplexityScore',dictionary=bv.dictionary), \n",
    "                                                                    artm.SparsityPhiScore(name='SparsityPhiScore'),\n",
    "                                                                    artm.SparsityThetaScore(name='SparsityThetaScore'),\n",
    "                                                                    artm.TopicKernelScore(name='TopicKernelScore',probability_mass_threshold=0.3), \n",
    "                                                                    artm.TopTokensScore(name='TopTokensScore', num_tokens=8)],\n",
    "                       regularizers=[artm.SmoothSparseThetaRegularizer(name='SparseTheta', tau=-0.4),\n",
    "                                     artm.DecorrelatorPhiRegularizer(name='DecorrelatorPhi', tau=2.5e+5)])\n",
    "                                     \n",
    "model_artm.num_document_passes = 4\n",
    "model_artm.initialize(bv.dictionary)\n",
    "model_artm.fit_offline(batch_vectorizer=bv, num_collection_passes=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_names = ['topic_{}'.format(i) for i in range(50)]\n",
    "\n",
    "model_artm1 = artm.ARTM(topic_names=topic_names,cache_theta=True, scores=[artm.PerplexityScore(name='PerplexityScore',dictionary=bv.dictionary), artm.SparsityPhiScore(name='SparsityPhiScore'),\n",
    "                                                                  artm.SparsityThetaScore(name='SparsityThetaScore'), \n",
    "                                                                  artm.TopicKernelScore(name='TopicKernelScore',probability_mass_threshold=0.3),\n",
    "                                                                  artm.TopTokensScore(name='TopTokensScore',num_tokens=12)],\n",
    "                        regularizers=[artm.SmoothSparseThetaRegularizer(name='SparseTheta', tau=-0.4),\n",
    "                        artm.SmoothSparsePhiRegularizer(name='SparsePhi',tau=-0.25),\n",
    "                        artm.DecorrelatorPhiRegularizer(name='DecorrelatorPhi', tau=2.5e+5)], seed=243) #seed is required for heirarchy \n",
    "                        \n",
    "model_artm1.num_document_passes = 4\n",
    "model_artm1.set_parent_model(parent_model = model_artm, parent_model_weight = 0.75)\n",
    "model_artm1.initialize(bv.dictionary)\n",
    "\n",
    "model_artm1.fit_offline(batch_vectorizer=bv, num_collection_passes=12)\n",
    "\n",
    "subt = pd.DataFrame(model_artm1.get_parent_psi())\n",
    "subt.columns = ['topic_{}'.format(i) for i in range(10)]\n",
    "subt.index = ['subtopic_{}'.format(i) for i in range(50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "subtopic_0     0.0\n",
       "subtopic_37    0.0\n",
       "subtopic_27    0.0\n",
       "subtopic_28    0.0\n",
       "subtopic_29    0.0\n",
       "Name: topic_2, dtype: float32"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def subtopics_wrt_topic(topic_number, matrix_dist):\n",
    "    return matrix_dist.iloc[:, topic_number].sort_values(ascending = False)[:5]\n",
    "subtopics_wrt_topic(2, subt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'theta'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-47ba1c663dbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0marticles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPaperText\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum_topics\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mget_articles_on_theme\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_texts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fully_cleaned_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-59-47ba1c663dbd>\u001b[0m in \u001b[0;36mget_articles_on_theme\u001b[0;34m(dataset, topic, num_topics)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_articles_on_theme\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtheta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_artm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_theta\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'topic_{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtheta\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0.05\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0midx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0marticles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtheta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0marticles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'theta'"
     ]
    }
   ],
   "source": [
    "def get_articles_on_theme(dataset, topic, num_topics):\n",
    "    theta = np.array(model_artm.get_theta('topic_{}'.format(topic)).iloc[0]).theta[theta <= 0.05] = 0\n",
    "    idx = np.nonzero(theta)[0]\n",
    "    articles = zip(idx, theta[idx])\n",
    "    articles = sorted(articles, key = lambda x: x[1], reverse = True)\n",
    "    articles = [x[0] for x in articles]\n",
    "    return dataset.iloc[articles].PaperText[:num_topics]\n",
    "    \n",
    "get_articles_on_theme(all_texts['fully_cleaned_text'], 8, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.8/site-packages/artm/batches_utils.py:227: DeprecationWarning: Please use `spmatrix` from the `scipy.sparse` namespace, the `scipy.sparse.base` namespace is deprecated.\n",
      "  from scipy.sparse.base import spmatrix\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #0: ['pluralityprocess compris', 'secondgroov by', 'storagea portion', 'locationstrip among', 'vehiclethe cure', 'claimaccord to', 'deviceshare by', 'associatedultraviolet light', 'nodea materi', 'responsesubstrat form']\n",
      "Topic #1: ['portionelectrod along', 'electrodecorrespond to', 'layerlayer correspond', 'materialare arrang', 'surfaceprocess the', 'metalsame layer', 'accordingdiffer materi', 'formingsubpixel or', 'claimaccord to', 'semiconductorcover the']\n",
      "Topic #2: ['elementand arrang', 'portionelectrod along', 'pluralityprocess compris', 'claimaccord to', 'elementsare space', 'secondgroov by', 'sensormateri the', 'componentlightemit function', 'structureclaim compris', 'comprisingupper surfac']\n",
      "Topic #3: ['outputbetween the', 'secondgroov by', 'terminaldevic compris', 'inputcompris form', 'currentsub organ', 'coupledwith each', 'connectedarrang along', 'circuitapart from', 'switchthe second', 'transistorthe ultraviolet']\n",
      "Topic #4: ['setlayer wherein', 'userportion wherein', 'basedmethod for', 'claimaccord to', 'methodelectrod space', 'targetwherein form', 'computernot equal', 'objectlayer face', 'pointsubstrat and', 'contentaway from']\n",
      "Topic #5: ['methodelectrod space', 'displayheat cure', 'claimaccord to', 'accordingdiffer materi', 'pluralityprocess compris', 'comprisingupper surfac', 'areaorgan lightemit', 'numberprovid a', 'sequenceplane the', 'processorlayer in']\n",
      "Topic #6: ['dataelectrod correspond', 'informationelectrod dispos', 'claimaccord to', 'electronicarrang in', 'userportion wherein', 'applicationonetoon manner', 'communicationplural of', 'messagesubpixel arrang', 'comprisingupper surfac', 'methodelectrod space']\n",
      "Topic #7: ['saidstrip dispos', 'claimaccord to', 'compositionfunction layer', 'cameraleast one', 'secondgroov by', 'lengthmanufactur the', 'havingwherein the', 'comprisesupper surfac', 'modulebetween two', 'pluralityprocess compris']\n",
      "Topic #8: ['claimaccord to', 'secondgroov by', 'layerlayer correspond', 'methodelectrod space', 'comprisingupper surfac', 'comprisesupper surfac', 'groupgroov correspond', 'regionother two', 'conductiveinsul with', 'materialare arrang']\n",
      "Topic #9: ['lightcompris a', 'surfaceprocess the', 'opticalthe extens', 'claimaccord to', 'housingsubstrat between', 'electricalaltern arrang', 'secondgroov by', 'accordingdiffer materi', 'bodydevic accord', 'axiswithout be']\n",
      "Topic #10: ['secondgroov by', 'signalprocess wherein', 'valuecompris the', 'controldispos on', 'configureddistanc between', 'claimaccord to', 'modeamong the', 'circuitapart from', 'timematrix the', 'dataelectrod correspond']\n",
      "Topic #11: ['secondgroov by', 'endadjac pixel', 'claimaccord to', 'portionelectrod along', 'memberportion face', 'surfaceprocess the', 'assemblytwo adjac', 'comprisingupper surfac', 'directionthe plural', 'fluidillumin by']\n",
      "Topic #12: ['deviceshare by', 'imageelectrod distanc', 'secondgroov by', 'processinggroov wherein', 'networkform a', 'claimaccord to', 'colorcure process', 'keyconstitut a', 'communicationplural of', 'configureddistanc between']\n",
      "Topic #13: ['powerlayer dispos', 'memorystrip and', 'liquidan organ', 'circuitapart from', 'cellsecond electrod', 'bitfirst electrod', 'controllerelectrod is', 'referenceform the', 'devicesare insul', 'signalprocess wherein']\n",
      "Topic #14: ['apparatusinkjet print', 'claimaccord to', 'secondgroov by', 'channelis share', 'positionandan organ', 'videoportion the', 'gasother and', 'timematrix the', 'comprisingupper surfac', 'methodelectrod space']\n"
     ]
    }
   ],
   "source": [
    "batch_vectorizer  = artm.BatchVectorizer(data_format='bow_n_wd',\n",
    "                          n_wd=n_wd[:len(vocabulary)],\n",
    "                          vocabulary=vocabulary)\n",
    "\n",
    "lda = artm.LDA(num_topics=15, alpha=0.01, beta=0.001, cache_theta=True, num_document_passes=5, dictionary=batch_vectorizer.dictionary)\n",
    "lda.fit_offline(batch_vectorizer=batch_vectorizer, num_collection_passes=10)\n",
    "\n",
    "top_tokens = lda.get_top_tokens(num_tokens=10)\n",
    "\n",
    "for i, token_list in enumerate(top_tokens):\n",
    "\n",
    "    print('Topic #{0}: {1}'.format(i, token_list))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                topic_0       topic_1       topic_2  \\\n",
      "andaextens direct          2.395767e-03  7.070442e-03  9.343595e-03   \n",
      "axiswithout be             3.097794e-06  2.025067e-06  4.577391e-04   \n",
      "claimaccord to             4.947443e-02  3.900435e-02  5.205112e-02   \n",
      "comprisesupper surfac      2.198097e-03  2.514996e-03  2.772527e-02   \n",
      "comprisingupper surfac     1.247424e-02  7.995741e-03  3.335467e-02   \n",
      "...                                 ...           ...           ...   \n",
      "gatefor cover              1.275919e-07  3.092150e-03  1.568412e-04   \n",
      "nodea materi               2.955038e-02  6.614570e-08  6.532024e-08   \n",
      "transistorthe ultraviolet  4.289479e-08  1.051301e-07  7.593392e-08   \n",
      "devicesare insul           5.030740e-04  4.361320e-07  7.676800e-05   \n",
      "valvecure the              6.263228e-08  8.307538e-08  5.910277e-08   \n",
      "\n",
      "                                topic_3       topic_4       topic_5  \\\n",
      "andaextens direct          1.340505e-02  8.949421e-04  2.881870e-03   \n",
      "axiswithout be             6.013558e-08  4.089035e-08  1.441550e-07   \n",
      "claimaccord to             7.151325e-03  4.498482e-02  8.837971e-02   \n",
      "comprisesupper surfac      8.440255e-03  2.414743e-02  2.275438e-02   \n",
      "comprisingupper surfac     2.492228e-02  2.254789e-02  4.414985e-02   \n",
      "...                                 ...           ...           ...   \n",
      "gatefor cover              3.644832e-02  3.419839e-08  5.017808e-08   \n",
      "nodea materi               7.768480e-03  9.410065e-07  3.459264e-07   \n",
      "transistorthe ultraviolet  4.326094e-02  3.154106e-08  4.332999e-08   \n",
      "devicesare insul           6.807970e-06  1.136968e-03  8.075804e-04   \n",
      "valvecure the              6.853039e-08  3.215644e-08  4.802031e-08   \n",
      "\n",
      "                                topic_6       topic_7       topic_8  \\\n",
      "andaextens direct          1.409978e-03  3.828966e-04  4.122690e-03   \n",
      "axiswithout be             3.777642e-08  3.966740e-04  1.638954e-03   \n",
      "claimaccord to             3.586391e-02  5.486787e-02  1.382630e-01   \n",
      "comprisesupper surfac      1.151484e-02  1.607716e-02  5.035564e-02   \n",
      "comprisingupper surfac     2.250607e-02  1.373542e-02  6.545401e-02   \n",
      "...                                 ...           ...           ...   \n",
      "gatefor cover              3.915029e-08  7.136628e-08  3.355114e-03   \n",
      "nodea materi               1.480611e-07  8.785432e-07  4.547781e-08   \n",
      "transistorthe ultraviolet  3.194200e-08  6.857769e-08  4.172978e-07   \n",
      "devicesare insul           2.390283e-03  2.147439e-06  7.759734e-08   \n",
      "valvecure the              3.233561e-08  2.027457e-06  5.148126e-08   \n",
      "\n",
      "                                topic_9      topic_10      topic_11  \\\n",
      "andaextens direct          7.012209e-03  5.204421e-03  1.128542e-02   \n",
      "axiswithout be             2.475605e-02  7.915845e-08  1.266991e-02   \n",
      "claimaccord to             5.641101e-02  4.063639e-02  5.940055e-02   \n",
      "comprisesupper surfac      1.608399e-02  7.245020e-03  7.483253e-03   \n",
      "comprisingupper surfac     1.788018e-02  7.286001e-03  2.356357e-02   \n",
      "...                                 ...           ...           ...   \n",
      "gatefor cover              5.810447e-08  7.221925e-07  2.748769e-08   \n",
      "nodea materi               4.871189e-08  3.049344e-06  2.396981e-08   \n",
      "transistorthe ultraviolet  5.419420e-08  4.000291e-08  2.450557e-08   \n",
      "devicesare insul           6.427493e-08  5.306924e-06  7.722299e-08   \n",
      "valvecure the              2.443729e-07  9.661952e-08  1.312140e-02   \n",
      "\n",
      "                               topic_12      topic_13      topic_14  \n",
      "andaextens direct          3.275309e-03  8.312642e-03  2.415441e-03  \n",
      "axiswithout be             1.156420e-05  9.118357e-08  2.850472e-07  \n",
      "claimaccord to             3.217025e-02  2.336969e-02  7.002275e-02  \n",
      "comprisesupper surfac      5.716099e-03  1.052493e-02  1.274830e-02  \n",
      "comprisingupper surfac     1.048589e-02  2.670629e-02  3.493368e-02  \n",
      "...                                 ...           ...           ...  \n",
      "gatefor cover              6.897674e-08  3.815547e-05  5.935604e-08  \n",
      "nodea materi               5.079904e-08  6.743521e-04  1.323024e-07  \n",
      "transistorthe ultraviolet  4.837749e-08  9.914613e-07  5.972536e-08  \n",
      "devicesare insul           1.169361e-02  2.850837e-02  2.027224e-07  \n",
      "valvecure the              4.455911e-08  5.523390e-04  9.179755e-08  \n",
      "\n",
      "[214 rows x 15 columns]\n",
      "-----------------------------\n",
      "              1000      1001      1002      1003      1004      1005  \\\n",
      "topic_0   0.000192  0.136183  0.000282  0.145451  0.000142  0.000152   \n",
      "topic_1   0.134035  0.000083  0.000280  0.000065  0.000169  0.000082   \n",
      "topic_2   0.028992  0.050704  0.000357  0.000113  0.000152  0.000109   \n",
      "topic_3   0.000199  0.000073  0.000272  0.000062  0.000120  0.000173   \n",
      "topic_4   0.101317  0.002093  0.000343  0.275199  0.000156  0.080397   \n",
      "topic_5   0.000228  0.341502  0.217552  0.116836  0.000189  0.380224   \n",
      "topic_6   0.000188  0.000048  0.000290  0.367467  0.000127  0.000227   \n",
      "topic_7   0.000240  0.000020  0.388317  0.085073  0.573217  0.000087   \n",
      "topic_8   0.113223  0.000021  0.317136  0.000493  0.324288  0.000214   \n",
      "topic_9   0.006749  0.000020  0.000326  0.000068  0.092498  0.000174   \n",
      "topic_10  0.000271  0.000287  0.000288  0.000162  0.000135  0.274032   \n",
      "topic_11  0.613678  0.000043  0.000322  0.000074  0.000363  0.058261   \n",
      "topic_12  0.000199  0.468419  0.000276  0.000086  0.000132  0.000890   \n",
      "topic_13  0.000211  0.000481  0.073052  0.000092  0.008153  0.000103   \n",
      "topic_14  0.000279  0.000022  0.000907  0.008760  0.000157  0.204875   \n",
      "\n",
      "              1006      1007      1008      1009  ...      990       991   \\\n",
      "topic_0   0.000424  0.012252  0.061638  0.049507  ...  0.281897  0.061655   \n",
      "topic_1   0.006478  0.183762  0.000105  0.000417  ...  0.000050  0.000024   \n",
      "topic_2   0.000185  0.122429  0.000110  0.056262  ...  0.000445  0.000025   \n",
      "topic_3   0.001773  0.384544  0.286176  0.039609  ...  0.000038  0.847631   \n",
      "topic_4   0.000294  0.000031  0.095186  0.000056  ...  0.000377  0.041682   \n",
      "topic_5   0.501135  0.000033  0.000084  0.287008  ...  0.017839  0.003401   \n",
      "topic_6   0.000212  0.000031  0.000047  0.207872  ...  0.553761  0.011675   \n",
      "topic_7   0.153844  0.000034  0.000047  0.004821  ...  0.000045  0.000020   \n",
      "topic_8   0.258264  0.228834  0.020375  0.000056  ...  0.000063  0.013723   \n",
      "topic_9   0.029770  0.000073  0.000189  0.007819  ...  0.000043  0.000030   \n",
      "topic_10  0.000393  0.000038  0.534462  0.015447  ...  0.000094  0.000042   \n",
      "topic_11  0.000264  0.058097  0.000075  0.000883  ...  0.000048  0.000031   \n",
      "topic_12  0.000178  0.009772  0.000158  0.218722  ...  0.144104  0.000056   \n",
      "topic_13  0.046278  0.000030  0.001230  0.003208  ...  0.000095  0.003545   \n",
      "topic_14  0.000509  0.000042  0.000117  0.108313  ...  0.001101  0.016459   \n",
      "\n",
      "              992       993       994       995       996       997   \\\n",
      "topic_0   0.015484  0.000133  0.000306  0.008515  0.001775  0.000143   \n",
      "topic_1   0.000018  0.114559  0.000230  0.000383  0.000128  0.137931   \n",
      "topic_2   0.020014  0.183171  0.000224  0.000200  0.625852  0.000119   \n",
      "topic_3   0.025357  0.000175  0.000361  0.132257  0.000138  0.000078   \n",
      "topic_4   0.014470  0.000159  0.076359  0.105640  0.027350  0.000140   \n",
      "topic_5   0.136898  0.178805  0.218058  0.370441  0.017430  0.039552   \n",
      "topic_6   0.239814  0.000133  0.000227  0.000233  0.000781  0.000098   \n",
      "topic_7   0.500742  0.035627  0.000315  0.000161  0.000103  0.000089   \n",
      "topic_8   0.000024  0.273832  0.237283  0.044131  0.000169  0.431702   \n",
      "topic_9   0.021032  0.000322  0.000243  0.019836  0.000140  0.000445   \n",
      "topic_10  0.005793  0.000154  0.003028  0.112682  0.000173  0.000094   \n",
      "topic_11  0.000092  0.047727  0.462521  0.008453  0.325393  0.064195   \n",
      "topic_12  0.011838  0.164879  0.000241  0.000092  0.000321  0.112345   \n",
      "topic_13  0.007755  0.000151  0.000280  0.196765  0.000094  0.001258   \n",
      "topic_14  0.000670  0.000171  0.000323  0.000208  0.000153  0.211810   \n",
      "\n",
      "              998       999   \n",
      "topic_0   0.000030  0.026676  \n",
      "topic_1   0.000176  0.000071  \n",
      "topic_2   0.121335  0.000061  \n",
      "topic_3   0.000119  0.000055  \n",
      "topic_4   0.000089  0.735134  \n",
      "topic_5   0.216486  0.000131  \n",
      "topic_6   0.000039  0.188940  \n",
      "topic_7   0.000027  0.000171  \n",
      "topic_8   0.000041  0.002134  \n",
      "topic_9   0.000040  0.000048  \n",
      "topic_10  0.620988  0.000050  \n",
      "topic_11  0.000598  0.000083  \n",
      "topic_12  0.039931  0.045592  \n",
      "topic_13  0.000030  0.000044  \n",
      "topic_14  0.000070  0.000811  \n",
      "\n",
      "[15 rows x 1402 columns]\n"
     ]
    }
   ],
   "source": [
    "phi = lda.phi_   # size is number of words in vocab x number of topics\n",
    "theta = lda.get_theta() # number of rows correspond to the number of topics\n",
    "\n",
    "print(phi)\n",
    "print(\"-----------------------------\")\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[209.13845825195312,\n",
       " 172.80743408203125,\n",
       " 111.44688415527344,\n",
       " 123.51750183105469,\n",
       " 72.13981628417969,\n",
       " 71.07655334472656,\n",
       " 68.26617431640625,\n",
       " 67.97388458251953,\n",
       " 67.94954681396484,\n",
       " 67.74874877929688,\n",
       " 67.69065856933594,\n",
       " 67.67171478271484,\n",
       " 67.65945434570312,\n",
       " 67.65250396728516,\n",
       " 67.64883422851562]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_artm = artm.ARTM(num_topics=15, cache_theta=True, scores=[artm.PerplexityScore(name='PerplexityScore', dictionary=batch_vectorizer.dictionary)], regularizers=[artm.SmoothSparseThetaRegularizer(name='SparseTheta', tau=-0.15)])\n",
    "\n",
    "#model_plsa.scores.add(artm.TopTokensScore(name='TopTokensScore', num_tokens=6))\n",
    "\n",
    "model_artm.scores.add(artm.SparsityPhiScore(name='SparsityPhiScore'))\n",
    "\n",
    "model_artm.scores.add(artm.TopicKernelScore(name='TopicKernelScore', probability_mass_threshold=0.3))\n",
    "\n",
    "model_artm.scores.add(artm.TopTokensScore(name='TopTokensScore', num_tokens=6))\n",
    "\n",
    "model_artm.regularizers.add(artm.SmoothSparsePhiRegularizer(name='SparsePhi', tau=-0.1))\n",
    "\n",
    "model_artm.regularizers.add(artm.DecorrelatorPhiRegularizer(name='DecorrelatorPhi', tau=1.5e+5))\n",
    "\n",
    "model_artm.num_document_passes = 1\n",
    "\n",
    "model_artm.initialize(dictionary=batch_vectorizer.dictionary)\n",
    "model_artm.fit_offline(batch_vectorizer=batch_vectorizer, num_collection_passes=15)\n",
    "\n",
    "model_artm.score_tracker['PerplexityScore'].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic_0:  ['storagea portion', 'pathlayer and', 'servicefirst portion']\n",
      "topic_1:  ['materialare arrang', 'substrateof subpixel', 'electrodecorrespond to', 'formedare altern', 'uppersubpixel a', 'conductiveinsul with']\n",
      "topic_2:  ['elementsare space', 'gatefor cover', 'signalselectrod are', 'adjacentdisplay devic', 'functionstrip are', 'distanceprocess andor']\n",
      "topic_3:  ['coupledwith each', 'currentsub organ', 'sourcelayer form', 'responsesubstrat form', 'voltagelayer the', 'supportsubpixel and']\n",
      "topic_4:  ['selectedthe pixel', 'transmissionwherein a', 'keyconstitut a', 'rateform an', 'configurationclaim wherein']\n",
      "topic_5:  ['areaorgan lightemit', 'valuesother the', 'pixelwherein upper', 'coverwhich are', 'blockeach other', 'colorcure process']\n",
      "topic_6:  ['computernot equal', 'targetwherein form', 'opticalthe extens']\n",
      "topic_7:  ['lengthmanufactur the', 'generatedgroov andform', 'portadjac first', 'compositionfunction layer', 'chamberinsul portion', 'cameraleast one']\n",
      "topic_8:  ['havingwherein the', 'andaextens direct', 'disposedprint process', 'modulebetween two', 'structureclaim compris', 'axiswithout be']\n",
      "topic_9:  ['directionthe plural', 'electricalaltern arrang', 'mainthe substrat', 'positionedsubstrat wherein']\n",
      "topic_10:  ['locationstrip among', 'numberprovid a', 'instructionsportion which', 'predeterminedlayer further', 'operationalong the', 'thresholdportion are']\n",
      "topic_11:  ['vehiclethe cure', 'sensormateri the', 'assemblytwo adjac', 'componentlightemit function', 'fluidillumin by', 'outerand upper']\n",
      "topic_12:  ['regionother two', 'contentaway from', 'frequencydirect of', 'messagesubpixel arrang', 'andtheface away', 'andwhereininject ink']\n",
      "topic_13:  ['memorystrip and', 'statesubpixel each', 'cellsecond electrod', 'liquidan organ', 'movementcolumn of', 'bitfirst electrod']\n",
      "topic_14:  ['channelis share', 'rangeare dispos', 'videoportion the', 'sequenceplane the', 'gasother and', 'causeone insul']\n"
     ]
    }
   ],
   "source": [
    "for topic_name in model_artm.topic_names:\n",
    "\n",
    "    print(topic_name + ': ',model_artm.score_tracker['TopTokensScore'].last_tokens[topic_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           topic_0  topic_1   topic_2   topic_3  topic_4  \\\n",
      "andaextens direct              0.0      0.0  0.000000  0.000000      0.0   \n",
      "axiswithout be                 0.0      0.0  0.000000  0.000000      0.0   \n",
      "claimaccord to                 0.0      0.0  0.000000  0.000000      0.0   \n",
      "comprisesupper surfac          0.0      0.0  0.000000  0.000000      0.0   \n",
      "comprisingupper surfac         0.0      0.0  0.000000  0.000000      0.0   \n",
      "...                            ...      ...       ...       ...      ...   \n",
      "gatefor cover                  0.0      0.0  0.163221  0.000000      0.0   \n",
      "nodea materi                   0.0      0.0  0.000000  0.075932      0.0   \n",
      "transistorthe ultraviolet      0.0      0.0  0.000000  0.072956      0.0   \n",
      "devicesare insul               0.0      0.0  0.000000  0.000000      0.0   \n",
      "valvecure the                  0.0      0.0  0.000000  0.000000      0.0   \n",
      "\n",
      "                           topic_5  topic_6  topic_7   topic_8  topic_9  \\\n",
      "andaextens direct              0.0      0.0      0.0  0.090674      0.0   \n",
      "axiswithout be                 0.0      0.0      0.0  0.054153      0.0   \n",
      "claimaccord to                 0.0      0.0      0.0  0.000000      0.0   \n",
      "comprisesupper surfac          0.0      0.0      0.0  0.000000      0.0   \n",
      "comprisingupper surfac         0.0      0.0      0.0  0.000000      0.0   \n",
      "...                            ...      ...      ...       ...      ...   \n",
      "gatefor cover                  0.0      0.0      0.0  0.000000      0.0   \n",
      "nodea materi                   0.0      0.0      0.0  0.000000      0.0   \n",
      "transistorthe ultraviolet      0.0      0.0      0.0  0.000000      0.0   \n",
      "devicesare insul               0.0      0.0      0.0  0.000000      0.0   \n",
      "valvecure the                  0.0      0.0      0.0  0.000000      0.0   \n",
      "\n",
      "                           topic_10  topic_11  topic_12  topic_13  topic_14  \n",
      "andaextens direct               0.0   0.00000       0.0       0.0       0.0  \n",
      "axiswithout be                  0.0   0.00000       0.0       0.0       0.0  \n",
      "claimaccord to                  0.0   0.00000       0.0       0.0       0.0  \n",
      "comprisesupper surfac           0.0   0.00000       0.0       0.0       0.0  \n",
      "comprisingupper surfac          0.0   0.00000       0.0       0.0       0.0  \n",
      "...                             ...       ...       ...       ...       ...  \n",
      "gatefor cover                   0.0   0.00000       0.0       0.0       0.0  \n",
      "nodea materi                    0.0   0.00000       0.0       0.0       0.0  \n",
      "transistorthe ultraviolet       0.0   0.00000       0.0       0.0       0.0  \n",
      "devicesare insul                0.0   0.00000       0.0       0.0       0.0  \n",
      "valvecure the                   0.0   0.04369       0.0       0.0       0.0  \n",
      "\n",
      "[214 rows x 15 columns]\n",
      "-----------------------------\n",
      "              1000      1001      1002      1003      1004      1005  \\\n",
      "topic_0   0.000000  0.000000  0.000000  0.092532  0.000000  0.000000   \n",
      "topic_1   0.319968  0.000000  0.000000  0.000000  0.061870  0.000000   \n",
      "topic_2   0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "topic_3   0.263797  0.013503  0.000000  0.000000  0.000000  0.000000   \n",
      "topic_4   0.000000  0.000000  0.035052  0.060065  0.039269  0.215624   \n",
      "topic_5   0.000000  0.235207  0.000000  0.000000  0.000000  0.000000   \n",
      "topic_6   0.000000  0.000000  0.000000  0.125000  0.000000  0.139966   \n",
      "topic_7   0.000000  0.000000  0.488660  0.000000  0.803421  0.000000   \n",
      "topic_8   0.315612  0.175725  0.158763  0.079739  0.077398  0.132249   \n",
      "topic_9   0.100624  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "topic_10  0.000000  0.459117  0.000000  0.157468  0.000000  0.000000   \n",
      "topic_11  0.000000  0.010136  0.000000  0.007924  0.000000  0.446752   \n",
      "topic_12  0.000000  0.000000  0.000000  0.449675  0.000000  0.065409   \n",
      "topic_13  0.000000  0.013503  0.117526  0.000000  0.018042  0.000000   \n",
      "topic_14  0.000000  0.092808  0.200000  0.027597  0.000000  0.000000   \n",
      "\n",
      "              1006      1007      1008      1009  ...      990       991   \\\n",
      "topic_0   0.000000  0.000000  0.010712  0.000000  ...  0.000000  0.000000   \n",
      "topic_1   0.096090  0.253355  0.078507  0.285852  ...  0.000000  0.000000   \n",
      "topic_2   0.000000  0.297279  0.098929  0.000000  ...  0.000000  0.013186   \n",
      "topic_3   0.200000  0.102667  0.224953  0.035611  ...  0.064189  0.878563   \n",
      "topic_4   0.000000  0.000000  0.023314  0.000000  ...  0.000000  0.006424   \n",
      "topic_5   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.000000   \n",
      "topic_6   0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.006424   \n",
      "topic_7   0.407792  0.003376  0.000000  0.189605  ...  0.000000  0.000000   \n",
      "topic_8   0.200014  0.206101  0.234817  0.151107  ...  0.000000  0.049622   \n",
      "topic_9   0.000000  0.070894  0.010712  0.000000  ...  0.000000  0.000000   \n",
      "topic_10  0.000000  0.011319  0.124134  0.112608  ...  0.897523  0.010932   \n",
      "topic_11  0.000000  0.000000  0.006468  0.151107  ...  0.019144  0.012985   \n",
      "topic_12  0.000000  0.055008  0.151538  0.000000  ...  0.000000  0.015440   \n",
      "topic_13  0.096104  0.000000  0.035917  0.074110  ...  0.019144  0.000000   \n",
      "topic_14  0.000000  0.000000  0.000000  0.000000  ...  0.000000  0.006424   \n",
      "\n",
      "              992       993       994       995       996       997   \\\n",
      "topic_0   0.050228  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "topic_1   0.000000  0.192646  0.000000  0.145027  0.041451  0.149407   \n",
      "topic_2   0.000000  0.000000  0.000000  0.000000  0.242188  0.000000   \n",
      "topic_3   0.011089  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "topic_4   0.024136  0.000000  0.036093  0.043257  0.000000  0.065310   \n",
      "topic_5   0.000000  0.000000  0.000000  0.043257  0.041295  0.000000   \n",
      "topic_6   0.154599  0.000000  0.000000  0.094148  0.041295  0.000000   \n",
      "topic_7   0.272016  0.000000  0.000000  0.043257  0.000000  0.000000   \n",
      "topic_8   0.149458  0.482980  0.163482  0.308505  0.197142  0.323801   \n",
      "topic_9   0.024136  0.000000  0.000000  0.000000  0.219866  0.000000   \n",
      "topic_10  0.128506  0.000000  0.000000  0.195929  0.000000  0.065310   \n",
      "topic_11  0.185832  0.324375  0.800425  0.083363  0.175470  0.095919   \n",
      "topic_12  0.000000  0.000000  0.000000  0.000000  0.041295  0.000000   \n",
      "topic_13  0.000000  0.000000  0.000000  0.043257  0.000000  0.167091   \n",
      "topic_14  0.000000  0.000000  0.000000  0.000000  0.000000  0.133164   \n",
      "\n",
      "              998       999   \n",
      "topic_0   0.000000  0.038752  \n",
      "topic_1   0.040287  0.260156  \n",
      "topic_2   0.245163  0.000000  \n",
      "topic_3   0.272447  0.018621  \n",
      "topic_4   0.000000  0.058883  \n",
      "topic_5   0.308900  0.008409  \n",
      "topic_6   0.026633  0.139406  \n",
      "topic_7   0.000000  0.000000  \n",
      "topic_8   0.022122  0.048853  \n",
      "topic_9   0.000000  0.000000  \n",
      "topic_10  0.000000  0.109408  \n",
      "topic_11  0.031181  0.058883  \n",
      "topic_12  0.012975  0.219930  \n",
      "topic_13  0.000000  0.000000  \n",
      "topic_14  0.040291  0.038700  \n",
      "\n",
      "[15 rows x 1402 columns]\n"
     ]
    }
   ],
   "source": [
    "phi_artm = model_artm.phi_   # size is number of words in vocab x number of topics\n",
    "theta_artm = model_artm.get_theta() # number of rows correspond to the number of topics\n",
    "\n",
    "print(phi_artm)\n",
    "print(\"-----------------------------\")\n",
    "print(theta_artm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[209.06007385253906,\n",
       " 141.7535400390625,\n",
       " 95.81549072265625,\n",
       " 104.95429229736328,\n",
       " 116.63621520996094,\n",
       " 116.48223114013672,\n",
       " 116.48223114013672,\n",
       " 116.48223114013672,\n",
       " 116.48223114013672,\n",
       " 116.48223114013672,\n",
       " 116.48223114013672,\n",
       " 116.48223114013672,\n",
       " 116.48223114013672,\n",
       " 116.48223114013672,\n",
       " 116.48223114013672,\n",
       " 116.48223114013672,\n",
       " 116.48223114013672,\n",
       " 116.48223114013672,\n",
       " 116.48223114013672,\n",
       " 116.48223114013672]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_artm = artm.ARTM(num_topics=15, cache_theta=True, scores=[artm.PerplexityScore(name='PerplexityScore', dictionary=batch_vectorizer.dictionary)], regularizers=[artm.SmoothSparseThetaRegularizer(name='SparseTheta', tau=-0.4)])\n",
    "\n",
    "#model_plsa.scores.add(artm.TopTokensScore(name='TopTokensScore', num_tokens=6))\n",
    "\n",
    "model_artm.scores.add(artm.SparsityPhiScore(name='SparsityPhiScore'))\n",
    "\n",
    "model_artm.scores.add(artm.TopicKernelScore(name='TopicKernelScore', probability_mass_threshold=0.3))\n",
    "\n",
    "model_artm.scores.add(artm.TopTokensScore(name='TopTokensScore', num_tokens=6))\n",
    "\n",
    "model_artm.regularizers.add(artm.SmoothSparsePhiRegularizer(name='SparsePhi', tau=-0.4))\n",
    "\n",
    "model_artm.regularizers.add(artm.DecorrelatorPhiRegularizer(name='DecorrelatorPhi', tau=2.5e+5))\n",
    "\n",
    "model_artm.num_document_passes = 1\n",
    "\n",
    "model_artm.initialize(dictionary=batch_vectorizer.dictionary)\n",
    "model_artm.fit_offline(batch_vectorizer=batch_vectorizer, num_collection_passes=20)\n",
    "\n",
    "model_artm.score_tracker['PerplexityScore'].value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cooc_dict = artm.Dictionary()\n",
    "# cooc_dict.gather(\n",
    "#     data_path='batches_folder',\n",
    "#     cooc_file_path='cooc.txt',\n",
    "#     vocab_file_path='vocab.txt',\n",
    "#     symmetric_cooc_values=True)\n",
    "\n",
    "# coherence_score = artm.TopTokensScore(\n",
    "#                             name='TopTokensCoherenceScore',\n",
    "#                             class_id='@default_class',\n",
    "#                             num_tokens=10,\n",
    "#                             topic_names=[u'topic_0',u'topic_1'],\n",
    "#                             dictionary=cooc_dict)\n",
    "\n",
    "# model_artm.scores.add(coherence_score)\n",
    "# model.score_tracker['TopTokensCoherenceScore'].average_coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
