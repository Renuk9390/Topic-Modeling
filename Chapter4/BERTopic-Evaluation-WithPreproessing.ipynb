{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03a66130",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grant_id</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USPP030977</td>\n",
       "      <td>A new and distinct variety of Mango plant, her...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USPP030978</td>\n",
       "      <td>&amp;#x2018;Honeysuckle Rose #1-6&amp;#x2019; is a new...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USPP030979</td>\n",
       "      <td>A new and distinct peach tree variety, &lt;i&gt;Prun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USPP030980</td>\n",
       "      <td>This invention relates to a new and distinct v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USPP030981</td>\n",
       "      <td>A new and distinct cultivar of Strawberry plan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7008</th>\n",
       "      <td>US10462947</td>\n",
       "      <td>Provided are a first component holding tool op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7009</th>\n",
       "      <td>US10462948</td>\n",
       "      <td>In a case in which mounting deviation is occur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7010</th>\n",
       "      <td>US10462949</td>\n",
       "      <td>A reel holding device is provided. The device ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7011</th>\n",
       "      <td>US10462950</td>\n",
       "      <td>An electronic component bonding device include...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7012</th>\n",
       "      <td>US10462951</td>\n",
       "      <td>A control device that determines tape referenc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7013 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        grant_id                                           abstract\n",
       "0     USPP030977  A new and distinct variety of Mango plant, her...\n",
       "1     USPP030978  &#x2018;Honeysuckle Rose #1-6&#x2019; is a new...\n",
       "2     USPP030979  A new and distinct peach tree variety, <i>Prun...\n",
       "3     USPP030980  This invention relates to a new and distinct v...\n",
       "4     USPP030981  A new and distinct cultivar of Strawberry plan...\n",
       "...          ...                                                ...\n",
       "7008  US10462947  Provided are a first component holding tool op...\n",
       "7009  US10462948  In a case in which mounting deviation is occur...\n",
       "7010  US10462949  A reel holding device is provided. The device ...\n",
       "7011  US10462950  An electronic component bonding device include...\n",
       "7012  US10462951  A control device that determines tape referenc...\n",
       "\n",
       "[7013 rows x 2 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#small dataset\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "\n",
    "usptodata = pd.read_csv('U.S. Patents.csv')\n",
    "usptodataset=usptodata[[\"grant_id\",\"claims_text\",\"abstract\"]]\n",
    "usptodataset= usptodata.dropna()\n",
    "US_Patent_df = usptodataset.reset_index(drop=True)\n",
    "US_Patent_df= US_Patent_df[['grant_id','abstract']]\n",
    "US_Patent_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35dbe98f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\dtypes\\inference.py:177: FutureWarning: Possible nested set at position 1\n",
      "  re.compile(obj)\n"
     ]
    }
   ],
   "source": [
    "def cleantext(df): \n",
    "    \n",
    "    US_Patent_df['cleaned_text'] = US_Patent_df['abstract'].replace(r'\\'|\\\"|\\,|\\.|\\?|\\+|\\-|\\/|\\=|\\(|\\)|\\n|\"', '', regex=True)\n",
    "    \n",
    "    # convert to lowercase\n",
    "    US_Patent_df['cleaned_text'] = US_Patent_df['cleaned_text'].str.lower()\n",
    "    \n",
    "    #remove numbers\n",
    "    US_Patent_df['cleaned_text'] =US_Patent_df['cleaned_text'].replace(r'\\d+', '', regex = True)\n",
    "        \n",
    "    #remove_symbols\n",
    "    US_Patent_df['cleaned_text']  = US_Patent_df['cleaned_text'].replace(r'[^a-zA-Z0-9]', \" \", regex=True)\n",
    "    \n",
    "    #remove punctuations \n",
    "    US_Patent_df['cleaned_text'] = US_Patent_df['cleaned_text'].replace(r'[[]!\"#$%\\'()\\*+,-./:;<=>?^_`{|}]+',\"\", regex = True)\n",
    "    \n",
    "    #remove_URL(x):\n",
    "    US_Patent_df['cleaned_text']  = US_Patent_df['cleaned_text'].replace(r'https.*$', \"\", regex = True)\n",
    "    US_Patent_df['cleaned_text'] = US_Patent_df['cleaned_text'].replace(\"   \", \" \", regex = True)\n",
    "    US_Patent_df['cleaned_text'] = US_Patent_df['cleaned_text'].replace(\"  \", \" \", regex = True)\n",
    "   \n",
    "    return US_Patent_df\n",
    "\n",
    "df = cleantext(US_Patent_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7078a928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grant_id</th>\n",
       "      <th>abstract</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_text_without_Stopwprd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USPP030977</td>\n",
       "      <td>A new and distinct variety of Mango plant, her...</td>\n",
       "      <td>a new and distinct variety of mango plant here...</td>\n",
       "      <td>new distinct variety mango plant herein referr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USPP030978</td>\n",
       "      <td>&amp;#x2018;Honeysuckle Rose #1-6&amp;#x2019; is a new...</td>\n",
       "      <td>x honeysuckle rose x is a new variety derived...</td>\n",
       "      <td>x honeysuckle rose x new variety derived x sim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USPP030979</td>\n",
       "      <td>A new and distinct peach tree variety, &lt;i&gt;Prun...</td>\n",
       "      <td>a new and distinct peach tree variety i prunus...</td>\n",
       "      <td>new distinct peach tree variety prunus persica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USPP030980</td>\n",
       "      <td>This invention relates to a new and distinct v...</td>\n",
       "      <td>this invention relates to a new and distinct v...</td>\n",
       "      <td>invention relates new distinct variety red ras...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USPP030981</td>\n",
       "      <td>A new and distinct cultivar of Strawberry plan...</td>\n",
       "      <td>a new and distinct cultivar of strawberry plan...</td>\n",
       "      <td>new distinct cultivar strawberry plant named x...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7008</th>\n",
       "      <td>US10462947</td>\n",
       "      <td>Provided are a first component holding tool op...</td>\n",
       "      <td>provided are a first component holding tool op...</td>\n",
       "      <td>provided first component holding tool operatin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7009</th>\n",
       "      <td>US10462948</td>\n",
       "      <td>In a case in which mounting deviation is occur...</td>\n",
       "      <td>in a case in which mounting deviation is occur...</td>\n",
       "      <td>case mounting deviation occurring component mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7010</th>\n",
       "      <td>US10462949</td>\n",
       "      <td>A reel holding device is provided. The device ...</td>\n",
       "      <td>a reel holding device is provided the device i...</td>\n",
       "      <td>reel holding device provided device includes c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7011</th>\n",
       "      <td>US10462950</td>\n",
       "      <td>An electronic component bonding device include...</td>\n",
       "      <td>an electronic component bonding device include...</td>\n",
       "      <td>electronic component bonding device includes m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7012</th>\n",
       "      <td>US10462951</td>\n",
       "      <td>A control device that determines tape referenc...</td>\n",
       "      <td>a control device that determines tape referenc...</td>\n",
       "      <td>control device determines tape reference posit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7013 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        grant_id                                           abstract  \\\n",
       "0     USPP030977  A new and distinct variety of Mango plant, her...   \n",
       "1     USPP030978  &#x2018;Honeysuckle Rose #1-6&#x2019; is a new...   \n",
       "2     USPP030979  A new and distinct peach tree variety, <i>Prun...   \n",
       "3     USPP030980  This invention relates to a new and distinct v...   \n",
       "4     USPP030981  A new and distinct cultivar of Strawberry plan...   \n",
       "...          ...                                                ...   \n",
       "7008  US10462947  Provided are a first component holding tool op...   \n",
       "7009  US10462948  In a case in which mounting deviation is occur...   \n",
       "7010  US10462949  A reel holding device is provided. The device ...   \n",
       "7011  US10462950  An electronic component bonding device include...   \n",
       "7012  US10462951  A control device that determines tape referenc...   \n",
       "\n",
       "                                           cleaned_text  \\\n",
       "0     a new and distinct variety of mango plant here...   \n",
       "1      x honeysuckle rose x is a new variety derived...   \n",
       "2     a new and distinct peach tree variety i prunus...   \n",
       "3     this invention relates to a new and distinct v...   \n",
       "4     a new and distinct cultivar of strawberry plan...   \n",
       "...                                                 ...   \n",
       "7008  provided are a first component holding tool op...   \n",
       "7009  in a case in which mounting deviation is occur...   \n",
       "7010  a reel holding device is provided the device i...   \n",
       "7011  an electronic component bonding device include...   \n",
       "7012  a control device that determines tape referenc...   \n",
       "\n",
       "                          cleaned_text_without_Stopwprd  \n",
       "0     new distinct variety mango plant herein referr...  \n",
       "1     x honeysuckle rose x new variety derived x sim...  \n",
       "2     new distinct peach tree variety prunus persica...  \n",
       "3     invention relates new distinct variety red ras...  \n",
       "4     new distinct cultivar strawberry plant named x...  \n",
       "...                                                 ...  \n",
       "7008  provided first component holding tool operatin...  \n",
       "7009  case mounting deviation occurring component mo...  \n",
       "7010  reel holding device provided device includes c...  \n",
       "7011  electronic component bonding device includes m...  \n",
       "7012  control device determines tape reference posit...  \n",
       "\n",
       "[7013 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#remove stopwords and words_to_remove\n",
    "import nltk\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "df['cleaned_text_without_Stopwprd'] = df['cleaned_text'].apply(lambda x: ' '.join([word for word in str(x).split() if word not in stopwords]))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7135d70f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grant_id</th>\n",
       "      <th>abstract</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>cleaned_text_without_Stopwprd</th>\n",
       "      <th>Lemmatization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USPP030977</td>\n",
       "      <td>A new and distinct variety of Mango plant, her...</td>\n",
       "      <td>a new and distinct variety of mango plant here...</td>\n",
       "      <td>new distinct variety mango plant herein referr...</td>\n",
       "      <td>new distinct variety mango plant herein referr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USPP030978</td>\n",
       "      <td>&amp;#x2018;Honeysuckle Rose #1-6&amp;#x2019; is a new...</td>\n",
       "      <td>x honeysuckle rose x is a new variety derived...</td>\n",
       "      <td>x honeysuckle rose x new variety derived x sim...</td>\n",
       "      <td>x honeysuckle rose x new variety derived x sim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USPP030979</td>\n",
       "      <td>A new and distinct peach tree variety, &lt;i&gt;Prun...</td>\n",
       "      <td>a new and distinct peach tree variety i prunus...</td>\n",
       "      <td>new distinct peach tree variety prunus persica...</td>\n",
       "      <td>new distinct peach tree variety prunus persica...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USPP030980</td>\n",
       "      <td>This invention relates to a new and distinct v...</td>\n",
       "      <td>this invention relates to a new and distinct v...</td>\n",
       "      <td>invention relates new distinct variety red ras...</td>\n",
       "      <td>invention relates new distinct variety red ras...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USPP030981</td>\n",
       "      <td>A new and distinct cultivar of Strawberry plan...</td>\n",
       "      <td>a new and distinct cultivar of strawberry plan...</td>\n",
       "      <td>new distinct cultivar strawberry plant named x...</td>\n",
       "      <td>new distinct cultivar strawberry plant named x...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7008</th>\n",
       "      <td>US10462947</td>\n",
       "      <td>Provided are a first component holding tool op...</td>\n",
       "      <td>provided are a first component holding tool op...</td>\n",
       "      <td>provided first component holding tool operatin...</td>\n",
       "      <td>provided first component holding tool operatin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7009</th>\n",
       "      <td>US10462948</td>\n",
       "      <td>In a case in which mounting deviation is occur...</td>\n",
       "      <td>in a case in which mounting deviation is occur...</td>\n",
       "      <td>case mounting deviation occurring component mo...</td>\n",
       "      <td>case mounting deviation occurring component mo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7010</th>\n",
       "      <td>US10462949</td>\n",
       "      <td>A reel holding device is provided. The device ...</td>\n",
       "      <td>a reel holding device is provided the device i...</td>\n",
       "      <td>reel holding device provided device includes c...</td>\n",
       "      <td>reel holding device provided device includes c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7011</th>\n",
       "      <td>US10462950</td>\n",
       "      <td>An electronic component bonding device include...</td>\n",
       "      <td>an electronic component bonding device include...</td>\n",
       "      <td>electronic component bonding device includes m...</td>\n",
       "      <td>electronic component bonding device includes m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7012</th>\n",
       "      <td>US10462951</td>\n",
       "      <td>A control device that determines tape referenc...</td>\n",
       "      <td>a control device that determines tape referenc...</td>\n",
       "      <td>control device determines tape reference posit...</td>\n",
       "      <td>control device determines tape reference posit...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7013 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        grant_id                                           abstract  \\\n",
       "0     USPP030977  A new and distinct variety of Mango plant, her...   \n",
       "1     USPP030978  &#x2018;Honeysuckle Rose #1-6&#x2019; is a new...   \n",
       "2     USPP030979  A new and distinct peach tree variety, <i>Prun...   \n",
       "3     USPP030980  This invention relates to a new and distinct v...   \n",
       "4     USPP030981  A new and distinct cultivar of Strawberry plan...   \n",
       "...          ...                                                ...   \n",
       "7008  US10462947  Provided are a first component holding tool op...   \n",
       "7009  US10462948  In a case in which mounting deviation is occur...   \n",
       "7010  US10462949  A reel holding device is provided. The device ...   \n",
       "7011  US10462950  An electronic component bonding device include...   \n",
       "7012  US10462951  A control device that determines tape referenc...   \n",
       "\n",
       "                                           cleaned_text  \\\n",
       "0     a new and distinct variety of mango plant here...   \n",
       "1      x honeysuckle rose x is a new variety derived...   \n",
       "2     a new and distinct peach tree variety i prunus...   \n",
       "3     this invention relates to a new and distinct v...   \n",
       "4     a new and distinct cultivar of strawberry plan...   \n",
       "...                                                 ...   \n",
       "7008  provided are a first component holding tool op...   \n",
       "7009  in a case in which mounting deviation is occur...   \n",
       "7010  a reel holding device is provided the device i...   \n",
       "7011  an electronic component bonding device include...   \n",
       "7012  a control device that determines tape referenc...   \n",
       "\n",
       "                          cleaned_text_without_Stopwprd  \\\n",
       "0     new distinct variety mango plant herein referr...   \n",
       "1     x honeysuckle rose x new variety derived x sim...   \n",
       "2     new distinct peach tree variety prunus persica...   \n",
       "3     invention relates new distinct variety red ras...   \n",
       "4     new distinct cultivar strawberry plant named x...   \n",
       "...                                                 ...   \n",
       "7008  provided first component holding tool operatin...   \n",
       "7009  case mounting deviation occurring component mo...   \n",
       "7010  reel holding device provided device includes c...   \n",
       "7011  electronic component bonding device includes m...   \n",
       "7012  control device determines tape reference posit...   \n",
       "\n",
       "                                          Lemmatization  \n",
       "0     new distinct variety mango plant herein referr...  \n",
       "1     x honeysuckle rose x new variety derived x sim...  \n",
       "2     new distinct peach tree variety prunus persica...  \n",
       "3     invention relates new distinct variety red ras...  \n",
       "4     new distinct cultivar strawberry plant named x...  \n",
       "...                                                 ...  \n",
       "7008  provided first component holding tool operatin...  \n",
       "7009  case mounting deviation occurring component mo...  \n",
       "7010  reel holding device provided device includes c...  \n",
       "7011  electronic component bonding device includes m...  \n",
       "7012  control device determines tape reference posit...  \n",
       "\n",
       "[7013 rows x 5 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_words(text):\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "df[\"Lemmatization\"] = df[\"cleaned_text_without_Stopwprd\"].apply(lambda text: lemmatize_words(text))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b50eae0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>330</td>\n",
       "      <td>0_composition_acid_invention_protein</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>202</td>\n",
       "      <td>1_semiconductor_layer_gate_substrate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>181</td>\n",
       "      <td>2_distal_patient_implant_valve</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>180</td>\n",
       "      <td>3_wireless_station_communication_transmission</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>136</td>\n",
       "      <td>4_audio_sound_acoustic_signal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>88</td>\n",
       "      <td>5_memory_storage_data_nonvolatile</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>86</td>\n",
       "      <td>6_image_object_pixel_feature</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>78</td>\n",
       "      <td>7_ink_print_printing_printer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>68</td>\n",
       "      <td>8_image_imaging_pixel_photoelectric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9</td>\n",
       "      <td>67</td>\n",
       "      <td>9_turbine_blade_compressor_fan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic  Count                                           Name\n",
       "1       0    330           0_composition_acid_invention_protein\n",
       "2       1    202           1_semiconductor_layer_gate_substrate\n",
       "3       2    181                 2_distal_patient_implant_valve\n",
       "4       3    180  3_wireless_station_communication_transmission\n",
       "5       4    136                  4_audio_sound_acoustic_signal\n",
       "6       5     88              5_memory_storage_data_nonvolatile\n",
       "7       6     86                   6_image_object_pixel_feature\n",
       "8       7     78                   7_ink_print_printing_printer\n",
       "9       8     68            8_image_imaging_pixel_photoelectric\n",
       "10      9     67                 9_turbine_blade_compressor_fan"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Default BERTopic\n",
    "from bertopic import BERTopic\n",
    "\n",
    "\n",
    "topic_model = BERTopic()\n",
    "\n",
    "# Run BERTopic model\n",
    "topics, probabilities = topic_model.fit_transform(df['Lemmatization']) \n",
    "\n",
    "topic_model.get_topic_info()[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3bddaf5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_V is:  0.6243368581014651\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "documents = pd.DataFrame({\"Document\": df['Lemmatization'],\n",
    "                          \"ID\": range(len(df['Lemmatization'])),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_v')\n",
    "coherence_CV= coherence_model.get_coherence()\n",
    "print(\"C_V is: \", coherence_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30f3af28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_mass is:  -0.9544792751367572\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "documents = pd.DataFrame({\"Document\": df['Lemmatization'],\n",
    "                          \"ID\": range(len(df['Lemmatization'])),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='u_mass')\n",
    "coherence_u_mass= coherence_model.get_coherence()\n",
    "print(\"u_mass is: \", coherence_u_mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5385a6d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_npmi is:  0.07380269273789511\n"
     ]
    }
   ],
   "source": [
    "documents = pd.DataFrame({\"Document\": df['Lemmatization'],\n",
    "                          \"ID\": range(len(df['Lemmatization'])),\n",
    "                          \"Topic\": topics})\n",
    "\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_npmi')\n",
    "coherence_c_nmpi = coherence_model.get_coherence()\n",
    "print(\"c_npmi is: \",coherence_c_nmpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ea19685",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Document Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "001e4014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SBert\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "                  \n",
    "# Initiate BERTopic\n",
    "topic_model = BERTopic(embedding_model=sentence_model)\n",
    "\n",
    "# Run BERTopic model\n",
    "topics, probabilities = topic_model.fit_transform(df['Lemmatization']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53b1ab3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_V is:  0.6004768397665675\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "documents = pd.DataFrame({\"Document\": df['Lemmatization'],\n",
    "                          \"ID\": range(len(df['Lemmatization'])),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_v')\n",
    "coherence_CV= coherence_model.get_coherence()\n",
    "print(\"C_V is: \", coherence_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "38c324fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_mass is:  -0.9704211308814226\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "documents = pd.DataFrame({\"Document\": df['Lemmatization'],\n",
    "                          \"ID\": range(len(df['Lemmatization'])),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='u_mass')\n",
    "coherence_u_mass= coherence_model.get_coherence()\n",
    "print(\"u_mass is: \", coherence_u_mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2906ea63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_npmi is:  0.057678850487438926\n"
     ]
    }
   ],
   "source": [
    "documents = pd.DataFrame({\"Document\": df['Lemmatization'],\n",
    "                          \"ID\": range(len(df['Lemmatization'])),\n",
    "                          \"Topic\": topics})\n",
    "\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_npmi')\n",
    "coherence_c_nmpi = coherence_model.get_coherence()\n",
    "print(\"c_npmi is: \",coherence_c_nmpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da975c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flair Document RNN Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "98c4ceef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flair Document RNN Embeddings\n",
    "from flair.embeddings import WordEmbeddings, DocumentRNNEmbeddings\n",
    "\n",
    "glove_embedding = WordEmbeddings('glove')\n",
    "\n",
    "document_embeddings_RNN = DocumentRNNEmbeddings([glove_embedding])\n",
    "\n",
    "# Initiate BERTopic\n",
    "topic_model = BERTopic(embedding_model=document_embeddings_RNN)\n",
    "\n",
    "# Run BERTopic model\n",
    "topics, probabilities = topic_model.fit_transform(df['Lemmatization']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d5c7bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_V is:  0.3498839408248132\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "documents = pd.DataFrame({\"Document\": df['Lemmatization'],\n",
    "                          \"ID\": range(len(df['Lemmatization'])),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_v')\n",
    "coherence_CV= coherence_model.get_coherence()\n",
    "print(\"C_V is: \", coherence_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1997d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_mass is:  -1.4572713269025823\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "documents = pd.DataFrame({\"Document\": df['Lemmatization'],\n",
    "                          \"ID\": range(len(df['Lemmatization'])),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='u_mass')\n",
    "coherence_u_mass= coherence_model.get_coherence()\n",
    "print(\"u_mass is: \", coherence_u_mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c42936fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_npmi is:  -0.18550032149957651\n"
     ]
    }
   ],
   "source": [
    "documents = pd.DataFrame({\"Document\": df['Lemmatization'],\n",
    "                          \"ID\": range(len(df['Lemmatization'])),\n",
    "                          \"Topic\": topics})\n",
    "\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_npmi')\n",
    "coherence_c_nmpi = coherence_model.get_coherence()\n",
    "print(\"c_npmi is: \",coherence_c_nmpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f85213f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-Learn Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec5be6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-Learn Embeddings\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "pipe = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    TruncatedSVD(100)\n",
    ")\n",
    "\n",
    "topic_model = BERTopic(embedding_model=pipe)\n",
    "\n",
    "# Run BERTopic model\n",
    "topics, probabilities = topic_model.fit_transform(df['Lemmatization']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3f128254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_V is:  0.6071204813154665\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "documents = pd.DataFrame({\"Document\": df['Lemmatization'],\n",
    "                          \"ID\": range(len(df['Lemmatization'])),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_v')\n",
    "coherence_CV= coherence_model.get_coherence()\n",
    "print(\"C_V is: \", coherence_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63cbbf2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_mass is:  -0.846175729171979\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "documents = pd.DataFrame({\"Document\": df['Lemmatization'],\n",
    "                          \"ID\": range(len(df['Lemmatization'])),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='u_mass')\n",
    "coherence_u_mass= coherence_model.get_coherence()\n",
    "print(\"u_mass is: \", coherence_u_mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be6446d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_npmi is:  0.06970628271578386\n"
     ]
    }
   ],
   "source": [
    "documents = pd.DataFrame({\"Document\": df['Lemmatization'],\n",
    "                          \"ID\": range(len(df['Lemmatization'])),\n",
    "                          \"Topic\": topics})\n",
    "\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_npmi')\n",
    "coherence_c_nmpi = coherence_model.get_coherence()\n",
    "print(\"c_npmi is: \",coherence_c_nmpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27a5c47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word + Document Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "56808e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word + Document Embeddings\n",
    "\n",
    "from bertopic.backend import WordDocEmbedder\n",
    "import gensim.downloader as api\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Word embedding model\n",
    "ft = api.load('fasttext-wiki-news-subwords-300')\n",
    "\n",
    "# Document embedding model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create a model that uses both language models and pass it through BERTopic\n",
    "word_doc_embedder = WordDocEmbedder(embedding_model=embedding_model, word_embedding_model=ft)\n",
    "topic_model = BERTopic(embedding_model=word_doc_embedder)\n",
    "\n",
    "# Run BERTopic model\n",
    "topics, probabilities = topic_model.fit_transform(df['Lemmatization']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d057475f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_V is:  0.6103206880583883\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "documents = pd.DataFrame({\"Document\": df['Lemmatization'],\n",
    "                          \"ID\": range(len(df['Lemmatization'])),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_v')\n",
    "coherence_CV= coherence_model.get_coherence()\n",
    "print(\"C_V is: \", coherence_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b8f019c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_mass is:  -0.9328423539179824\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "documents = pd.DataFrame({\"Document\": df['Lemmatization'],\n",
    "                          \"ID\": range(len(df['Lemmatization'])),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='u_mass')\n",
    "coherence_u_mass= coherence_model.get_coherence()\n",
    "print(\"u_mass is: \", coherence_u_mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6672834e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_npmi is:  0.05757822149379966\n"
     ]
    }
   ],
   "source": [
    "documents = pd.DataFrame({\"Document\": df['Lemmatization'],\n",
    "                          \"ID\": range(len(df['Lemmatization'])),\n",
    "                          \"Topic\": topics})\n",
    "\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_npmi')\n",
    "coherence_c_nmpi = coherence_model.get_coherence()\n",
    "print(\"c_npmi is: \",coherence_c_nmpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "73a48fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dimention Reduction hyperparameter Tunung n_neighbors=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "57ce1565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_neighbors\n",
    "from bertopic import BERTopic\n",
    "import plotly.express as px\n",
    "# Dimension reduction\n",
    "from umap import UMAP\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# Initiate UMAP\n",
    "umap_model = UMAP(n_neighbors=5, \n",
    "                  n_components=5, \n",
    "                  min_dist=0.0, \n",
    "                  metric='cosine', \n",
    "                  random_state=100)\n",
    "\n",
    "\n",
    "# Initiate BERTopic\n",
    "topic_model = BERTopic(umap_model=umap_model)\n",
    "\n",
    "# Run BERTopic model\n",
    "topics, probabilities = topic_model.fit_transform(df['Lemmatization']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3e4fe6c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_V is:  0.5647261677692204\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "documents = pd.DataFrame({\"Document\": df['Lemmatization'],\n",
    "                          \"ID\": range(len(df['Lemmatization'])),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_v')\n",
    "coherence_CV= coherence_model.get_coherence()\n",
    "print(\"C_V is: \", coherence_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d5dcc3c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_mass is:  -1.1479677312775007\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "documents = pd.DataFrame({\"Document\": df['Lemmatization'],\n",
    "                          \"ID\": range(len(df['Lemmatization'])),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='u_mass')\n",
    "coherence_u_mass= coherence_model.get_coherence()\n",
    "print(\"u_mass is: \", coherence_u_mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e90e0028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_npmi is:  0.013881566402297891\n"
     ]
    }
   ],
   "source": [
    "documents = pd.DataFrame({\"Document\": df['Lemmatization'],\n",
    "                          \"ID\": range(len(df['Lemmatization'])),\n",
    "                          \"Topic\": topics})\n",
    "\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_npmi')\n",
    "coherence_c_nmpi = coherence_model.get_coherence()\n",
    "print(\"c_npmi is: \",coherence_c_nmpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23c4e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dimention Reduction hyperparameter Tunung n_neighbors=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6365154f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_neighbors\n",
    "from bertopic import BERTopic\n",
    "import plotly.express as px\n",
    "# Dimension reduction\n",
    "from umap import UMAP\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# Initiate UMAP\n",
    "umap_model = UMAP(n_neighbors=20, \n",
    "                  n_components=5, \n",
    "                  min_dist=0.0, \n",
    "                  metric='cosine', \n",
    "                  random_state=100)\n",
    "\n",
    "\n",
    "# Initiate BERTopic\n",
    "topic_model = BERTopic(umap_model=umap_model)\n",
    "\n",
    "# Run BERTopic model\n",
    "topics, probabilities = topic_model.fit_transform(df['Lemmatization']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fbb31e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_V is:  0.6222736159135049\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "documents = pd.DataFrame({\"Document\": df['Lemmatization'],\n",
    "                          \"ID\": range(len(df['Lemmatization'])),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_v')\n",
    "coherence_CV= coherence_model.get_coherence()\n",
    "print(\"C_V is: \", coherence_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "00a3a70f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_mass is:  -0.9312123700821461\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "documents = pd.DataFrame({\"Document\": df['Lemmatization'],\n",
    "                          \"ID\": range(len(df['Lemmatization'])),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='u_mass')\n",
    "coherence_u_mass= coherence_model.get_coherence()\n",
    "print(\"u_mass is: \", coherence_u_mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ee760108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_npmi is:  0.06861853773836013\n"
     ]
    }
   ],
   "source": [
    "documents = pd.DataFrame({\"Document\": df['Lemmatization'],\n",
    "                          \"ID\": range(len(df['Lemmatization'])),\n",
    "                          \"Topic\": topics})\n",
    "\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_npmi')\n",
    "coherence_c_nmpi = coherence_model.get_coherence()\n",
    "print(\"c_npmi is: \",coherence_c_nmpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12dcd30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dimention Reduction hyperparameter Tunung n_neighbors=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bc7a06b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_neighbors\n",
    "from bertopic import BERTopic\n",
    "import plotly.express as px\n",
    "# Dimension reduction\n",
    "from umap import UMAP\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# Initiate UMAP\n",
    "umap_model = UMAP(n_neighbors=30, \n",
    "                  n_components=5, \n",
    "                  min_dist=0.0, \n",
    "                  metric='cosine', \n",
    "                  random_state=100)\n",
    "\n",
    "\n",
    "# Initiate BERTopic\n",
    "topic_model = BERTopic(umap_model=umap_model)\n",
    "\n",
    "# Run BERTopic model\n",
    "topics, probabilities = topic_model.fit_transform(df['Lemmatization']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bc02a599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_V is:  0.6201391605351443\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "documents = pd.DataFrame({\"Document\": df['Lemmatization'],\n",
    "                          \"ID\": range(len(df['Lemmatization'])),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_v')\n",
    "coherence_CV= coherence_model.get_coherence()\n",
    "print(\"C_V is: \", coherence_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "aac7c50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_mass is:  -0.8731474781300098\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "documents = pd.DataFrame({\"Document\": df['Lemmatization'],\n",
    "                          \"ID\": range(len(df['Lemmatization'])),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='u_mass')\n",
    "coherence_u_mass= coherence_model.get_coherence()\n",
    "print(\"u_mass is: \", coherence_u_mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c2cac7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_npmi is:  0.07283728989534546\n"
     ]
    }
   ],
   "source": [
    "documents = pd.DataFrame({\"Document\": df['Lemmatization'],\n",
    "                          \"ID\": range(len(df['Lemmatization'])),\n",
    "                          \"Topic\": topics})\n",
    "\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_npmi')\n",
    "coherence_c_nmpi = coherence_model.get_coherence()\n",
    "print(\"c_npmi is: \",coherence_c_nmpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "523e517a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9883a05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA\n",
    "from bertopic import BERTopic\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "dim_model = PCA(n_components=5)\n",
    "topic_model = BERTopic(umap_model=dim_model)\n",
    "\n",
    "# Run BERTopic model\n",
    "topics, probabilities = topic_model.fit_transform(df['Lemmatization']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "73a4827b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_V is:  0.46469771020684897\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "documents = pd.DataFrame({\"Document\": df['Lemmatization'],\n",
    "                          \"ID\": range(len(df['Lemmatization'])),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_v')\n",
    "coherence_CV= coherence_model.get_coherence()\n",
    "print(\"C_V is: \", coherence_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "04206f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_mass is:  -0.1208401464212525\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "documents = pd.DataFrame({\"Document\": df['Lemmatization'],\n",
    "                          \"ID\": range(len(df['Lemmatization'])),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='u_mass')\n",
    "coherence_u_mass= coherence_model.get_coherence()\n",
    "print(\"u_mass is: \", coherence_u_mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "86fc978e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_npmi is:  0.002428290795743252\n"
     ]
    }
   ],
   "source": [
    "documents = pd.DataFrame({\"Document\": df['Lemmatization'],\n",
    "                          \"ID\": range(len(df['Lemmatization'])),\n",
    "                          \"Topic\": topics})\n",
    "\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_npmi')\n",
    "coherence_c_nmpi = coherence_model.get_coherence()\n",
    "print(\"c_npmi is: \",coherence_c_nmpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5969313d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncated SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "11d39a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncated SVD\n",
    "from bertopic import BERTopic\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "dim_model = TruncatedSVD(n_components=5)\n",
    "topic_model = BERTopic(umap_model=dim_model)\n",
    "\n",
    "# Run BERTopic model\n",
    "topics, probabilities = topic_model.fit_transform(df['Lemmatization']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ea253918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_V is:  0.4844477321787683\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "documents = pd.DataFrame({\"Document\": df['Lemmatization'],\n",
    "                          \"ID\": range(len(df['Lemmatization'])),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_v')\n",
    "coherence_CV= coherence_model.get_coherence()\n",
    "print(\"C_V is: \", coherence_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "c73f6bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_mass is:  -0.03153617507402938\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "documents = pd.DataFrame({\"Document\": df['Lemmatization'],\n",
    "                          \"ID\": range(len(df['Lemmatization'])),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='u_mass')\n",
    "coherence_u_mass= coherence_model.get_coherence()\n",
    "print(\"u_mass is: \", coherence_u_mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7a00df70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_npmi is:  0.05463636744797781\n"
     ]
    }
   ],
   "source": [
    "documents = pd.DataFrame({\"Document\": df['Lemmatization'],\n",
    "                          \"ID\": range(len(df['Lemmatization'])),\n",
    "                          \"Topic\": topics})\n",
    "\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_npmi')\n",
    "coherence_c_nmpi = coherence_model.get_coherence()\n",
    "print(\"c_npmi is: \",coherence_c_nmpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "72a25c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustering Hyperparameter Tuning HDBSCAN(min_cluster_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "eba9dd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDBSCAN\n",
    "from bertopic import BERTopic\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "#sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initiate UMAP\n",
    "umap_model = UMAP(n_neighbors=30, \n",
    "                  n_components=5, \n",
    "                  min_dist=0.0, \n",
    "                  metric='cosine', \n",
    "                  random_state=100)\n",
    "\n",
    "\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=30,min_samples = 10, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "topic_model = BERTopic(umap_model=umap_model, hdbscan_model=hdbscan_model)\n",
    "\n",
    "# Run BERTopic model\n",
    "topics, probabilities = topic_model.fit_transform(df['Lemmatization']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2fdd3398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_V is:  0.6784506830671473\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "documents = pd.DataFrame({\"Document\": df['Lemmatization'],\n",
    "                          \"ID\": range(len(df['Lemmatization'])),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_v')\n",
    "coherence_CV= coherence_model.get_coherence()\n",
    "print(\"C_V is: \", coherence_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8e3a6dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDBSCAN\n",
    "from bertopic import BERTopic\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initiate UMAP\n",
    "umap_model = UMAP(n_neighbors=30, \n",
    "                  n_components=5, \n",
    "                  min_dist=0.0, \n",
    "                  metric='cosine', \n",
    "                  random_state=100)\n",
    "\n",
    "\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=30,min_samples = 10, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "topic_model = BERTopic(umap_model=umap_model, hdbscan_model=hdbscan_model, embedding_model=sentence_model)\n",
    "\n",
    "# Run BERTopic model\n",
    "topics, probabilities = topic_model.fit_transform(df['Lemmatization']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5285cedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_V is:  0.6784506830671473\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "documents = pd.DataFrame({\"Document\": df['Lemmatization'],\n",
    "                          \"ID\": range(len(df['Lemmatization'])),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_v')\n",
    "coherence_CV= coherence_model.get_coherence()\n",
    "print(\"C_V is: \", coherence_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3dd47f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_mass is:  -0.5382076000943807\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "documents = pd.DataFrame({\"Document\": df['Lemmatization'],\n",
    "                          \"ID\": range(len(df['Lemmatization'])),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='u_mass')\n",
    "coherence_u_mass= coherence_model.get_coherence()\n",
    "print(\"u_mass is: \", coherence_u_mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "b502a8fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_npmi is:  0.12713254987901704\n"
     ]
    }
   ],
   "source": [
    "documents = pd.DataFrame({\"Document\": df['Lemmatization'],\n",
    "                          \"ID\": range(len(df['Lemmatization'])),\n",
    "                          \"Topic\": topics})\n",
    "\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_npmi')\n",
    "coherence_c_nmpi = coherence_model.get_coherence()\n",
    "print(\"c_npmi is: \",coherence_c_nmpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30455cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6887a058",
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-mean\n",
    "from bertopic import BERTopic\n",
    "from sklearn.cluster import KMeans\n",
    "from umap import UMAP\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initiate UMAP\n",
    "umap_model = UMAP(n_neighbors=30, \n",
    "                  n_components=5, \n",
    "                  min_dist=0.0, \n",
    "                  metric='cosine', \n",
    "                  random_state=100)\n",
    "\n",
    "\n",
    "cluster_model = KMeans(n_clusters=30)\n",
    "topic_model = BERTopic(umap_model=umap_model, embedding_model=sentence_model, hdbscan_model=cluster_model)\n",
    "\n",
    "# Run BERTopic model\n",
    "topics, probabilities = topic_model.fit_transform(df['Lemmatization']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8843f842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_V is:  0.6597882029228616\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "documents = pd.DataFrame({\"Document\": df['Lemmatization'],\n",
    "                          \"ID\": range(len(df['Lemmatization'])),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_v')\n",
    "coherence_CV= coherence_model.get_coherence()\n",
    "print(\"C_V is: \", coherence_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1ee80d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_mass is:  -0.39993312204992176\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "documents = pd.DataFrame({\"Document\": df['Lemmatization'],\n",
    "                          \"ID\": range(len(df['Lemmatization'])),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='u_mass')\n",
    "coherence_u_mass= coherence_model.get_coherence()\n",
    "print(\"u_mass is: \", coherence_u_mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6a73d794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_npmi is:  0.1149160310020116\n"
     ]
    }
   ],
   "source": [
    "documents = pd.DataFrame({\"Document\": df['Lemmatization'],\n",
    "                          \"ID\": range(len(df['Lemmatization'])),\n",
    "                          \"Topic\": topics})\n",
    "\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_npmi')\n",
    "coherence_c_nmpi = coherence_model.get_coherence()\n",
    "print(\"c_npmi is: \",coherence_c_nmpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "42ad6ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter Tuning for Topic Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fadcf542",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set CountVetorizer, c-TF-IDF\n",
    "from bertopic import BERTopic\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initiate UMAP\n",
    "umap_model = UMAP(n_neighbors=30, \n",
    "                  n_components=5, \n",
    "                  min_dist=0.0, \n",
    "                  metric='cosine', \n",
    "                  random_state=100)\n",
    "\n",
    "#HDBSCAN\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=30, min_samples = 10, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "\n",
    "\n",
    "#CountVectorizer\n",
    "vectorizer_model = CountVectorizer(max_features=1_000, min_df=10, ngram_range=(1, 3))\n",
    "\n",
    "#c-TF-IDF\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "\n",
    "#Run Model\n",
    "topic_model = BERTopic(umap_model=umap_model,\n",
    "                       embedding_model=sentence_model, \n",
    "                       hdbscan_model=hdbscan_model, \n",
    "                       vectorizer_model=vectorizer_model,\n",
    "                       ctfidf_model=ctfidf_model,\n",
    "                       diversity=None)\n",
    "\n",
    "# Run BERTopic model\n",
    "topics, probabilities = topic_model.fit_transform(df['Lemmatization']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "bea6827d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_V is:  0.7218902419798914\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "documents = pd.DataFrame({\"Document\": df['Lemmatization'],\n",
    "                          \"ID\": range(len(df['Lemmatization'])),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_v')\n",
    "coherence_CV= coherence_model.get_coherence()\n",
    "print(\"C_V is: \", coherence_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8faa2545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_mass is:  -0.4451818699536958\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "documents = pd.DataFrame({\"Document\": df['Lemmatization'],\n",
    "                          \"ID\": range(len(df['Lemmatization'])),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='u_mass')\n",
    "coherence_u_mass= coherence_model.get_coherence()\n",
    "print(\"u_mass is: \", coherence_u_mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "23707cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_npmi is:  0.07945150878558065\n"
     ]
    }
   ],
   "source": [
    "documents = pd.DataFrame({\"Document\": df['Lemmatization'],\n",
    "                          \"ID\": range(len(df['Lemmatization'])),\n",
    "                          \"Topic\": topics})\n",
    "\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_npmi')\n",
    "coherence_c_nmpi = coherence_model.get_coherence()\n",
    "print(\"c_npmi is: \",coherence_c_nmpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "90fcd3d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1ffc9a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initiate UMAP\n",
    "umap_model = UMAP(n_neighbors=30, \n",
    "                  n_components=5, \n",
    "                  min_dist=0.0, \n",
    "                  metric='cosine', \n",
    "                  random_state=100)\n",
    "\n",
    "#HDBSCAN\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=30, min_samples = 10, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "\n",
    "\n",
    "#CountVectorizer\n",
    "vectorizer_model = CountVectorizer(max_features=1_000, min_df=10, ngram_range=(1, 3))\n",
    "\n",
    "#c-TF-IDF\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "\n",
    "#Run Model\n",
    "topic_model = BERTopic(umap_model=umap_model,\n",
    "                       embedding_model=sentence_model, \n",
    "                       hdbscan_model=hdbscan_model, \n",
    "                       vectorizer_model=vectorizer_model,\n",
    "                       ctfidf_model=ctfidf_model,\n",
    "                       diversity=0.8)\n",
    "\n",
    "# Run BERTopic model\n",
    "topics, probabilities = topic_model.fit_transform(df['Lemmatization']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3d867521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_V is:  0.5621607366760811\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "documents = pd.DataFrame({\"Document\": df['Lemmatization'],\n",
    "                          \"ID\": range(len(df['Lemmatization'])),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_v')\n",
    "coherence_CV= coherence_model.get_coherence()\n",
    "print(\"C_V is: \", coherence_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ec67cfa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_mass is:  -0.456673083144409\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "documents = pd.DataFrame({\"Document\": df['Lemmatization'],\n",
    "                          \"ID\": range(len(df['Lemmatization'])),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='u_mass')\n",
    "coherence_u_mass= coherence_model.get_coherence()\n",
    "print(\"u_mass is: \", coherence_u_mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "396a2597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_npmi is:  -0.11460071221947954\n"
     ]
    }
   ],
   "source": [
    "documents = pd.DataFrame({\"Document\": df['Lemmatization'],\n",
    "                          \"ID\": range(len(df['Lemmatization'])),\n",
    "                          \"Topic\": topics})\n",
    "\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_npmi')\n",
    "coherence_c_nmpi = coherence_model.get_coherence()\n",
    "print(\"c_npmi is: \",coherence_c_nmpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bb353f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Topic Visualization with Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "f6d7a51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initiate UMAP\n",
    "umap_model = UMAP(n_neighbors=30, \n",
    "                  n_components=5, \n",
    "                  min_dist=0.0, \n",
    "                  metric='cosine', \n",
    "                  random_state=100)\n",
    "\n",
    "#HDBSCAN\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=30, min_samples = 10, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "\n",
    "\n",
    "#CountVectorizer\n",
    "vectorizer_model = CountVectorizer(max_features=1_000, min_df=10, ngram_range=(1, 3))\n",
    "\n",
    "#c-TF-IDF\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "\n",
    "#Run Model\n",
    "topic_model = BERTopic(umap_model=umap_model,\n",
    "                       embedding_model=sentence_model, \n",
    "                       hdbscan_model=hdbscan_model, \n",
    "                       vectorizer_model=vectorizer_model,\n",
    "                       ctfidf_model=ctfidf_model,\n",
    "                       diversity=None)\n",
    "\n",
    "# Run BERTopic model\n",
    "topics, probabilities = topic_model.fit_transform(df['Lemmatization']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "263df47f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33/33 [00:00<00:00, 246.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "â”œâ”€valve_end_seat_shaft_assembly\n",
      "â”‚    â”œâ”€heat_combustion_gas_cooling_air\n",
      "â”‚    â”‚    â”œâ”€heat_combustion_gas_cooling_air\n",
      "â”‚    â”‚    â”‚    â”œâ”€â– â”€â”€combustion_gas_engine_pipe_internal â”€â”€ Topic: 31\n",
      "â”‚    â”‚    â”‚    â””â”€â– â”€â”€heat_cooling_air_protruding_thermal â”€â”€ Topic: 24\n",
      "â”‚    â”‚    â””â”€â– â”€â”€container_compartment_cap_said_delivery â”€â”€ Topic: 30\n",
      "â”‚    â””â”€valve_end_shaft_seat_distal\n",
      "â”‚         â”œâ”€cutting_connector_cable_tool_container\n",
      "â”‚         â”‚    â”œâ”€â– â”€â”€cutting_tool_edge_longitudinal_segment â”€â”€ Topic: 28\n",
      "â”‚         â”‚    â””â”€connector_cable_container_side wall_locking\n",
      "â”‚         â”‚         â”œâ”€â– â”€â”€connector_cable_conductor_locking_electrical â”€â”€ Topic: 20\n",
      "â”‚         â”‚         â””â”€â– â”€â”€container_side wall_wall_cover_bottom â”€â”€ Topic: 29\n",
      "â”‚         â””â”€valve_seat_shaft_distal_ring\n",
      "â”‚              â”œâ”€seat_shaft_ring_bearing_outer\n",
      "â”‚              â”‚    â”œâ”€seat_support_rear_door_front\n",
      "â”‚              â”‚    â”‚    â”œâ”€â– â”€â”€door_lock_locking_fastener_clamp â”€â”€ Topic: 16\n",
      "â”‚              â”‚    â”‚    â””â”€â– â”€â”€seat_support_rear_front_side â”€â”€ Topic: 12\n",
      "â”‚              â”‚    â””â”€ring_shaft_bearing_outer_radially\n",
      "â”‚              â”‚         â”œâ”€â– â”€â”€ring_shaft_bearing_outer_groove â”€â”€ Topic: 11\n",
      "â”‚              â”‚         â””â”€â– â”€â”€engine_cooling_air_shaft_casing â”€â”€ Topic: 17\n",
      "â”‚              â””â”€valve_distal_proximal_fluid_flow\n",
      "â”‚                   â”œâ”€â– â”€â”€valve_inlet_outlet_fluid_water â”€â”€ Topic: 13\n",
      "â”‚                   â””â”€â– â”€â”€distal_proximal_valve_tissue_instrument â”€â”€ Topic: 8\n",
      "â””â”€network_layer_data_semiconductor_image\n",
      "     â”œâ”€network_layer_semiconductor_data_image\n",
      "     â”‚    â”œâ”€network_image_data_user_node\n",
      "     â”‚    â”‚    â”œâ”€image_object_sample_apparatus_color\n",
      "     â”‚    â”‚    â”‚    â”œâ”€â– â”€â”€head_forming_image_recording_apparatus â”€â”€ Topic: 14\n",
      "     â”‚    â”‚    â”‚    â””â”€image_object_sample_feature_captured\n",
      "     â”‚    â”‚    â”‚         â”œâ”€â– â”€â”€frequency_signal_vibration_volume_parameter â”€â”€ Topic: 10\n",
      "     â”‚    â”‚    â”‚         â””â”€â– â”€â”€image_object_sample_color_captured â”€â”€ Topic: 5\n",
      "     â”‚    â”‚    â””â”€network_node_data_user_communication\n",
      "     â”‚    â”‚         â”œâ”€network_user_communication_access_data\n",
      "     â”‚    â”‚         â”‚    â”œâ”€network_access_service_communication_user\n",
      "     â”‚    â”‚         â”‚    â”‚    â”œâ”€â– â”€â”€wireless_station_communication_transmission_network â”€â”€ Topic: 3\n",
      "     â”‚    â”‚         â”‚    â”‚    â””â”€â– â”€â”€memory_network_storage_data_service â”€â”€ Topic: 0\n",
      "     â”‚    â”‚         â”‚    â””â”€â– â”€â”€vehicle_sensor_driving_safety_operator â”€â”€ Topic: 7\n",
      "     â”‚    â”‚         â””â”€â– â”€â”€voltage_converter_power_circuit_switch â”€â”€ Topic: 6\n",
      "     â”‚    â””â”€layer_composition_semiconductor_substrate_electrode\n",
      "     â”‚         â”œâ”€composition_sub_compound_invention_sub sub\n",
      "     â”‚         â”‚    â”œâ”€â– â”€â”€composition_invention_treatment_present invention_provides â”€â”€ Topic: 1\n",
      "     â”‚         â”‚    â””â”€battery_composition_sub_sub sub_polymer\n",
      "     â”‚         â”‚         â”œâ”€battery_composition_sub_polymer_sub sub\n",
      "     â”‚         â”‚         â”‚    â”œâ”€â– â”€â”€composition_polymer_resin_sub_fiber â”€â”€ Topic: 2\n",
      "     â”‚         â”‚         â”‚    â””â”€â– â”€â”€battery_electrode_positive_negative_secondary â”€â”€ Topic: 15\n",
      "     â”‚         â”‚         â””â”€â– â”€â”€pressure_fluid_flow_sensor_chamber â”€â”€ Topic: 26\n",
      "     â”‚         â””â”€semiconductor_layer_substrate_gate_touch\n",
      "     â”‚              â”œâ”€semiconductor_layer_gate_substrate_touch\n",
      "     â”‚              â”‚    â”œâ”€semiconductor_layer_gate_substrate_conductive\n",
      "     â”‚              â”‚    â”‚    â”œâ”€semiconductor_gate_layer_substrate_conductive\n",
      "     â”‚              â”‚    â”‚    â”‚    â”œâ”€â– â”€â”€semiconductor_gate_layer_conductive_substrate â”€â”€ Topic: 4\n",
      "     â”‚              â”‚    â”‚    â”‚    â””â”€â– â”€â”€display_panel_driving_line_liquid â”€â”€ Topic: 9\n",
      "     â”‚              â”‚    â”‚    â””â”€optical_light_light source_fiber_wavelength\n",
      "     â”‚              â”‚    â”‚         â”œâ”€optical_light_light source_fiber_wavelength\n",
      "     â”‚              â”‚    â”‚         â”‚    â”œâ”€â– â”€â”€light_light source_source_emitting_diode â”€â”€ Topic: 18\n",
      "     â”‚              â”‚    â”‚         â”‚    â””â”€â– â”€â”€optical_fiber_wavelength_index_port â”€â”€ Topic: 19\n",
      "     â”‚              â”‚    â”‚         â””â”€â– â”€â”€conductor_feed_ground_metal_door â”€â”€ Topic: 23\n",
      "     â”‚              â”‚    â””â”€touch_optical_sensing_screen_electrode\n",
      "     â”‚              â”‚         â”œâ”€â– â”€â”€optical_aperture_positive_axis_negative â”€â”€ Topic: 27\n",
      "     â”‚              â”‚         â””â”€â– â”€â”€touch_sensing_screen_electrode_panel â”€â”€ Topic: 25\n",
      "     â”‚              â””â”€â– â”€â”€emitting_nm_light_electrode_wiring â”€â”€ Topic: 33\n",
      "     â””â”€video_variety_derived_frame_producing\n",
      "          â”œâ”€video_frame_block_motion_color\n",
      "          â”‚    â”œâ”€â– â”€â”€video_frame_block_motion_color â”€â”€ Topic: 21\n",
      "          â”‚    â””â”€â– â”€â”€platform_disclosure_content_computing_disclosure relates â”€â”€ Topic: 32\n",
      "          â””â”€â– â”€â”€variety_derived_producing_invention also_produced â”€â”€ Topic: 22\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "hierarchical_topics = topic_model.hierarchical_topics(df['Lemmatization'])\n",
    "tree = topic_model.get_topic_tree(hierarchical_topics)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a3a021",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
