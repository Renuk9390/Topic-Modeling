{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4156bafd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>grant_id</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>USPP030977</td>\n",
       "      <td>A new and distinct variety of Mango plant, her...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>USPP030978</td>\n",
       "      <td>&amp;#x2018;Honeysuckle Rose #1-6&amp;#x2019; is a new...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>USPP030979</td>\n",
       "      <td>A new and distinct peach tree variety, &lt;i&gt;Prun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>USPP030980</td>\n",
       "      <td>This invention relates to a new and distinct v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>USPP030981</td>\n",
       "      <td>A new and distinct cultivar of Strawberry plan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7008</th>\n",
       "      <td>US10462947</td>\n",
       "      <td>Provided are a first component holding tool op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7009</th>\n",
       "      <td>US10462948</td>\n",
       "      <td>In a case in which mounting deviation is occur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7010</th>\n",
       "      <td>US10462949</td>\n",
       "      <td>A reel holding device is provided. The device ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7011</th>\n",
       "      <td>US10462950</td>\n",
       "      <td>An electronic component bonding device include...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7012</th>\n",
       "      <td>US10462951</td>\n",
       "      <td>A control device that determines tape referenc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7013 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        grant_id                                           abstract\n",
       "0     USPP030977  A new and distinct variety of Mango plant, her...\n",
       "1     USPP030978  &#x2018;Honeysuckle Rose #1-6&#x2019; is a new...\n",
       "2     USPP030979  A new and distinct peach tree variety, <i>Prun...\n",
       "3     USPP030980  This invention relates to a new and distinct v...\n",
       "4     USPP030981  A new and distinct cultivar of Strawberry plan...\n",
       "...          ...                                                ...\n",
       "7008  US10462947  Provided are a first component holding tool op...\n",
       "7009  US10462948  In a case in which mounting deviation is occur...\n",
       "7010  US10462949  A reel holding device is provided. The device ...\n",
       "7011  US10462950  An electronic component bonding device include...\n",
       "7012  US10462951  A control device that determines tape referenc...\n",
       "\n",
       "[7013 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#small dataset\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "\n",
    "usptodata = pd.read_csv('U.S. Patents.csv')\n",
    "usptodataset=usptodata[[\"grant_id\",\"claims_text\",\"abstract\"]]\n",
    "usptodataset= usptodata.dropna()\n",
    "US_Patent_df = usptodataset.reset_index(drop=True)\n",
    "df= US_Patent_df[['grant_id','abstract']]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36e46c08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Count</th>\n",
       "      <th>Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>311</td>\n",
       "      <td>0_compositions_invention_acid_methods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>242</td>\n",
       "      <td>1_wireless_ue_station_communication</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>217</td>\n",
       "      <td>2_semiconductor_layer_gate_substrate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>161</td>\n",
       "      <td>3_composition_catalyst_polymer_acid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>111</td>\n",
       "      <td>4_audio_sound_acoustic_speaker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>101</td>\n",
       "      <td>5_heat_cooling_air_exchanger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>87</td>\n",
       "      <td>6_display_panel_area_lines</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>7</td>\n",
       "      <td>86</td>\n",
       "      <td>7_content_search_language_document</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>8</td>\n",
       "      <td>74</td>\n",
       "      <td>8_composite_material_component_forming</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>9</td>\n",
       "      <td>69</td>\n",
       "      <td>9_voltage_power_converter_load</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic  Count                                    Name\n",
       "1       0    311   0_compositions_invention_acid_methods\n",
       "2       1    242     1_wireless_ue_station_communication\n",
       "3       2    217    2_semiconductor_layer_gate_substrate\n",
       "4       3    161     3_composition_catalyst_polymer_acid\n",
       "5       4    111          4_audio_sound_acoustic_speaker\n",
       "6       5    101            5_heat_cooling_air_exchanger\n",
       "7       6     87              6_display_panel_area_lines\n",
       "8       7     86      7_content_search_language_document\n",
       "9       8     74  8_composite_material_component_forming\n",
       "10      9     69          9_voltage_power_converter_load"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Default BERTopic\n",
    "from bertopic import BERTopic\n",
    "\n",
    "\n",
    "topic_model = BERTopic()\n",
    "\n",
    "# Run BERTopic model\n",
    "topics, probabilities = topic_model.fit_transform(df['abstract']) \n",
    "\n",
    "topic_model.get_topic_info()[1:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d51a8951",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_V is:  0.5790462775102138\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "documents = pd.DataFrame({\"Document\": df['abstract'],\n",
    "                          \"ID\": range(len(df['abstract'])),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_v')\n",
    "coherence_CV= coherence_model.get_coherence()\n",
    "print(\"C_V is: \", coherence_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d01ea46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_mass is:  -0.9942423314098539\n"
     ]
    }
   ],
   "source": [
    "documents = pd.DataFrame({\"Document\": df['abstract'],\n",
    "                          \"ID\": range(len(df['abstract'])),\n",
    "                          \"Topic\": topics})\n",
    "\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='u_mass')\n",
    "coherence_u_mass = coherence_model.get_coherence()\n",
    "print(\"u_mass is: \",coherence_u_mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d34b3a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_npmi is:  0.03070224968721843\n"
     ]
    }
   ],
   "source": [
    "documents = pd.DataFrame({\"Document\": df['abstract'],\n",
    "                          \"ID\": range(len(df['abstract'])),\n",
    "                          \"Topic\": topics})\n",
    "\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_npmi')\n",
    "coherence_c_nmpi = coherence_model.get_coherence()\n",
    "print(\"c_npmi is: \",coherence_c_nmpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1c034638",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Document Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f656a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SBert\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "                  \n",
    "# Initiate BERTopic\n",
    "topic_model = BERTopic(embedding_model=sentence_model)\n",
    "\n",
    "# Run BERTopic model\n",
    "topics, probabilities = topic_model.fit_transform(df['abstract']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c92c0e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_V is:  0.5776808930163991\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "documents = pd.DataFrame({\"Document\": df['abstract'],\n",
    "                          \"ID\": range(len(df['abstract'])),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_v')\n",
    "coherence_CV= coherence_model.get_coherence()\n",
    "print(\"C_V is: \", coherence_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e838384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_mass is:  -0.9987578856203695\n"
     ]
    }
   ],
   "source": [
    "documents = pd.DataFrame({\"Document\": df['abstract'],\n",
    "                          \"ID\": range(len(df['abstract'])),\n",
    "                          \"Topic\": topics})\n",
    "\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='u_mass')\n",
    "coherence_u_mass = coherence_model.get_coherence()\n",
    "print(\"u_mass is: \",coherence_u_mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df879269",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_npmi is:  0.02983151604335143\n"
     ]
    }
   ],
   "source": [
    "documents = pd.DataFrame({\"Document\": df['abstract'],\n",
    "                          \"ID\": range(len(df['abstract'])),\n",
    "                          \"Topic\": topics})\n",
    "\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_npmi')\n",
    "coherence_c_nmpi = coherence_model.get_coherence()\n",
    "print(\"c_npmi is: \",coherence_c_nmpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2534348e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flair Document RNN Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "184fb071",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Flair Document RNN Embeddings\n",
    "from flair.embeddings import WordEmbeddings, DocumentRNNEmbeddings\n",
    "\n",
    "glove_embedding = WordEmbeddings('glove')\n",
    "\n",
    "document_embeddings_RNN = DocumentRNNEmbeddings([glove_embedding])\n",
    "\n",
    "# Initiate BERTopic\n",
    "topic_model = BERTopic(embedding_model=document_embeddings_RNN)\n",
    "\n",
    "# Run BERTopic model\n",
    "topics, probabilities = topic_model.fit_transform(df['abstract']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ad79731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_V is:  0.3688731222285405\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "documents = pd.DataFrame({\"Document\": df['abstract'],\n",
    "                          \"ID\": range(len(df['abstract'])),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_v')\n",
    "coherence_CV= coherence_model.get_coherence()\n",
    "print(\"C_V is: \", coherence_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fbb2ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_mass is:  -1.3973022807541868\n"
     ]
    }
   ],
   "source": [
    "documents = pd.DataFrame({\"Document\": df['abstract'],\n",
    "                          \"ID\": range(len(df['abstract'])),\n",
    "                          \"Topic\": topics})\n",
    "\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='u_mass')\n",
    "coherence_u_mass = coherence_model.get_coherence()\n",
    "print(\"u_mass is: \",coherence_u_mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dddacf54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_npmi is:  -0.1686189482985781\n"
     ]
    }
   ],
   "source": [
    "documents = pd.DataFrame({\"Document\": df['abstract'],\n",
    "                          \"ID\": range(len(df['abstract'])),\n",
    "                          \"Topic\": topics})\n",
    "\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_npmi')\n",
    "coherence_c_nmpi = coherence_model.get_coherence()\n",
    "print(\"c_npmi is: \",coherence_c_nmpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5bda99b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-Learn Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5ea2e2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-Learn Embeddings\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "pipe = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    TruncatedSVD(100)\n",
    ")\n",
    "\n",
    "topic_model = BERTopic(embedding_model=pipe)\n",
    "\n",
    "# Run BERTopic model\n",
    "topics, probabilities = topic_model.fit_transform(df['abstract']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b68e6789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_V is:  0.5787289984553984\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "documents = pd.DataFrame({\"Document\": df['abstract'],\n",
    "                          \"ID\": range(len(df['abstract'])),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_v')\n",
    "coherence_CV= coherence_model.get_coherence()\n",
    "print(\"C_V is: \", coherence_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b28e2fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_mass is:  -0.9486350112854466\n"
     ]
    }
   ],
   "source": [
    "documents = pd.DataFrame({\"Document\": df['abstract'],\n",
    "                          \"ID\": range(len(df['abstract'])),\n",
    "                          \"Topic\": topics})\n",
    "\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='u_mass')\n",
    "coherence_u_mass = coherence_model.get_coherence()\n",
    "print(\"u_mass is: \",coherence_u_mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b59b7423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_npmi is:  0.03643839864634053\n"
     ]
    }
   ],
   "source": [
    "documents = pd.DataFrame({\"Document\": df['abstract'],\n",
    "                          \"ID\": range(len(df['abstract'])),\n",
    "                          \"Topic\": topics})\n",
    "\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_npmi')\n",
    "coherence_c_nmpi = coherence_model.get_coherence()\n",
    "print(\"c_npmi is: \",coherence_c_nmpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21473b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word + Document Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5cdba0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word + Document Embeddings\n",
    "\n",
    "from bertopic.backend import WordDocEmbedder\n",
    "import gensim.downloader as api\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Word embedding model\n",
    "ft = api.load('fasttext-wiki-news-subwords-300')\n",
    "\n",
    "# Document embedding model\n",
    "embedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create a model that uses both language models and pass it through BERTopic\n",
    "word_doc_embedder = WordDocEmbedder(embedding_model=embedding_model, word_embedding_model=ft)\n",
    "topic_model = BERTopic(embedding_model=word_doc_embedder)\n",
    "\n",
    "# Run BERTopic model\n",
    "topics, probabilities = topic_model.fit_transform(df['abstract']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c8de8c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_V is:  0.5830973504252722\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "documents = pd.DataFrame({\"Document\": df['abstract'],\n",
    "                          \"ID\": range(len(df['abstract'])),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_v')\n",
    "coherence_CV= coherence_model.get_coherence()\n",
    "print(\"C_V is: \", coherence_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9ae26b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_mass is:  -0.94175020479732\n"
     ]
    }
   ],
   "source": [
    "documents = pd.DataFrame({\"Document\": df['abstract'],\n",
    "                          \"ID\": range(len(df['abstract'])),\n",
    "                          \"Topic\": topics})\n",
    "\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='u_mass')\n",
    "coherence_u_mass = coherence_model.get_coherence()\n",
    "print(\"u_mass is: \",coherence_u_mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8161f99c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_npmi is:  0.04597434245096353\n"
     ]
    }
   ],
   "source": [
    "documents = pd.DataFrame({\"Document\": df['abstract'],\n",
    "                          \"ID\": range(len(df['abstract'])),\n",
    "                          \"Topic\": topics})\n",
    "\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_npmi')\n",
    "coherence_c_nmpi = coherence_model.get_coherence()\n",
    "print(\"c_npmi is: \",coherence_c_nmpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ac78579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dimention Reduction hyperparameter Tunung n_neighbors=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b9e9bce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_neighbors\n",
    "from bertopic import BERTopic\n",
    "import plotly.express as px\n",
    "# Dimension reduction\n",
    "from umap import UMAP\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# Initiate UMAP\n",
    "umap_model = UMAP(n_neighbors=5, \n",
    "                  n_components=5, \n",
    "                  min_dist=0.0, \n",
    "                  metric='cosine', \n",
    "                  random_state=100)\n",
    "\n",
    "\n",
    "# Initiate BERTopic\n",
    "topic_model = BERTopic(umap_model=umap_model)\n",
    "\n",
    "# Run BERTopic model\n",
    "topics, probabilities = topic_model.fit_transform(df['abstract']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f944488d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_V is:  0.5013702955528961\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "documents = pd.DataFrame({\"Document\": df['abstract'],\n",
    "                          \"ID\": range(len(df['abstract'])),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_v')\n",
    "coherence_CV= coherence_model.get_coherence()\n",
    "print(\"C_V is: \", coherence_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "85ddf9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_mass is:  -1.1670694887120743\n"
     ]
    }
   ],
   "source": [
    "documents = pd.DataFrame({\"Document\": df['abstract'],\n",
    "                          \"ID\": range(len(df['abstract'])),\n",
    "                          \"Topic\": topics})\n",
    "\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='u_mass')\n",
    "coherence_u_mass = coherence_model.get_coherence()\n",
    "print(\"u_mass is: \",coherence_u_mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7d068387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_npmi is:  -0.024187934840512\n"
     ]
    }
   ],
   "source": [
    "documents = pd.DataFrame({\"Document\": df['abstract'],\n",
    "                          \"ID\": range(len(df['abstract'])),\n",
    "                          \"Topic\": topics})\n",
    "\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_npmi')\n",
    "coherence_c_nmpi = coherence_model.get_coherence()\n",
    "print(\"c_npmi is: \",coherence_c_nmpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bc17ceb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dimention Reduction hyperparameter Tunung n_neighbors=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a1bc194f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_neighbors\n",
    "from bertopic import BERTopic\n",
    "import plotly.express as px\n",
    "# Dimension reduction\n",
    "from umap import UMAP\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# Initiate UMAP\n",
    "umap_model = UMAP(n_neighbors=20, \n",
    "                  n_components=5, \n",
    "                  min_dist=0.0, \n",
    "                  metric='cosine', \n",
    "                  random_state=100)\n",
    "\n",
    "\n",
    "# Initiate BERTopic\n",
    "topic_model = BERTopic(umap_model=umap_model)\n",
    "\n",
    "# Run BERTopic model\n",
    "topics, probabilities = topic_model.fit_transform(df['abstract']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4222b312",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_V is:  0.5863461790688316\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "documents = pd.DataFrame({\"Document\": df['abstract'],\n",
    "                          \"ID\": range(len(df['abstract'])),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_v')\n",
    "coherence_CV= coherence_model.get_coherence()\n",
    "print(\"C_V is: \", coherence_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9f3bd0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_mass is:  -0.9309368855585765\n"
     ]
    }
   ],
   "source": [
    "documents = pd.DataFrame({\"Document\": df['abstract'],\n",
    "                          \"ID\": range(len(df['abstract'])),\n",
    "                          \"Topic\": topics})\n",
    "\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='u_mass')\n",
    "coherence_u_mass = coherence_model.get_coherence()\n",
    "print(\"u_mass is: \",coherence_u_mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e9bb9658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_npmi is:  0.035621089038955306\n"
     ]
    }
   ],
   "source": [
    "documents = pd.DataFrame({\"Document\": df['abstract'],\n",
    "                          \"ID\": range(len(df['abstract'])),\n",
    "                          \"Topic\": topics})\n",
    "\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_npmi')\n",
    "coherence_c_nmpi = coherence_model.get_coherence()\n",
    "print(\"c_npmi is: \",coherence_c_nmpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "07c8a1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dimention Reduction hyperparameter Tunung n_neighbors=30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bfed8621",
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_neighbors\n",
    "from bertopic import BERTopic\n",
    "import plotly.express as px\n",
    "# Dimension reduction\n",
    "from umap import UMAP\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "\n",
    "# Initiate UMAP\n",
    "umap_model = UMAP(n_neighbors=30, \n",
    "                  n_components=5, \n",
    "                  min_dist=0.0, \n",
    "                  metric='cosine', \n",
    "                  random_state=100)\n",
    "\n",
    "\n",
    "# Initiate BERTopic\n",
    "topic_model = BERTopic(umap_model=umap_model)\n",
    "\n",
    "# Run BERTopic model\n",
    "topics, probabilities = topic_model.fit_transform(df['abstract'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2993fe32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_V is:  0.6091321743644926\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "documents = pd.DataFrame({\"Document\": df['abstract'],\n",
    "                          \"ID\": range(len(df['abstract'])),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_v')\n",
    "coherence_CV= coherence_model.get_coherence()\n",
    "print(\"C_V is: \", coherence_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bfcac883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_mass is:  -0.864861939881414\n"
     ]
    }
   ],
   "source": [
    "documents = pd.DataFrame({\"Document\": df['abstract'],\n",
    "                          \"ID\": range(len(df['abstract'])),\n",
    "                          \"Topic\": topics})\n",
    "\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='u_mass')\n",
    "coherence_u_mass = coherence_model.get_coherence()\n",
    "print(\"u_mass is: \",coherence_u_mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "54af5dfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_npmi is:  0.04944747004044802\n"
     ]
    }
   ],
   "source": [
    "documents = pd.DataFrame({\"Document\": df['abstract'],\n",
    "                          \"ID\": range(len(df['abstract'])),\n",
    "                          \"Topic\": topics})\n",
    "\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_npmi')\n",
    "coherence_c_nmpi = coherence_model.get_coherence()\n",
    "print(\"c_npmi is: \",coherence_c_nmpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8112c4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "90711940",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA\n",
    "from bertopic import BERTopic\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "dim_model = PCA(n_components=5)\n",
    "topic_model = BERTopic(umap_model=dim_model)\n",
    "\n",
    "# Run BERTopic model\n",
    "topics, probabilities = topic_model.fit_transform(df['abstract']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "78675727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_V is:  0.33397736306910086\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "documents = pd.DataFrame({\"Document\": df['abstract'],\n",
    "                          \"ID\": range(len(df['abstract'])),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_v')\n",
    "coherence_CV= coherence_model.get_coherence()\n",
    "print(\"C_V is: \", coherence_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9b274534",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_mass is:  1.0000889005818412e-12\n"
     ]
    }
   ],
   "source": [
    "documents = pd.DataFrame({\"Document\": df['abstract'],\n",
    "                          \"ID\": range(len(df['abstract'])),\n",
    "                          \"Topic\": topics})\n",
    "\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='u_mass')\n",
    "coherence_u_mass = coherence_model.get_coherence()\n",
    "print(\"u_mass is: \",coherence_u_mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a3991415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_npmi is:  -0.02117719297772275\n"
     ]
    }
   ],
   "source": [
    "documents = pd.DataFrame({\"Document\": df['abstract'],\n",
    "                          \"ID\": range(len(df['abstract'])),\n",
    "                          \"Topic\": topics})\n",
    "\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_npmi')\n",
    "coherence_c_nmpi = coherence_model.get_coherence()\n",
    "print(\"c_npmi is: \",coherence_c_nmpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0c32a184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncated SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fd1bfa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Truncated SVD\n",
    "from bertopic import BERTopic\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "dim_model = TruncatedSVD(n_components=5)\n",
    "topic_model = BERTopic(umap_model=dim_model)\n",
    "\n",
    "# Run BERTopic model\n",
    "topics, probabilities = topic_model.fit_transform(df['abstract']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "2e5bc055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_V is:  0.406916986487947\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "documents = pd.DataFrame({\"Document\": df['abstract'],\n",
    "                          \"ID\": range(len(df['abstract'])),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_v')\n",
    "coherence_CV= coherence_model.get_coherence()\n",
    "print(\"C_V is: \", coherence_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "8cbdb668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_mass is:  -0.10346056009858706\n"
     ]
    }
   ],
   "source": [
    "documents = pd.DataFrame({\"Document\": df['abstract'],\n",
    "                          \"ID\": range(len(df['abstract'])),\n",
    "                          \"Topic\": topics})\n",
    "\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='u_mass')\n",
    "coherence_u_mass = coherence_model.get_coherence()\n",
    "print(\"u_mass is: \",coherence_u_mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0b497817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_npmi is:  0.018397746967304947\n"
     ]
    }
   ],
   "source": [
    "documents = pd.DataFrame({\"Document\": df['abstract'],\n",
    "                          \"ID\": range(len(df['abstract'])),\n",
    "                          \"Topic\": topics})\n",
    "\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_npmi')\n",
    "coherence_c_nmpi = coherence_model.get_coherence()\n",
    "print(\"c_npmi is: \",coherence_c_nmpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f611d26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clustering Hyperparameter Tuning HDBSCAN(min_cluster_size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "efd585ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HDBSCAN\n",
    "from bertopic import BERTopic\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initiate UMAP\n",
    "umap_model = UMAP(n_neighbors=30, \n",
    "                  n_components=5, \n",
    "                  min_dist=0.0, \n",
    "                  metric='cosine', \n",
    "                  random_state=100)\n",
    "\n",
    "\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=30,min_samples = 10, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "topic_model = BERTopic(umap_model=umap_model, hdbscan_model=hdbscan_model,embedding_model=sentence_model)\n",
    "\n",
    "# Run BERTopic model\n",
    "topics, probabilities = topic_model.fit_transform(df['abstract']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "01df9437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_V is:  0.6040011226893\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "documents = pd.DataFrame({\"Document\": df['abstract'],\n",
    "                          \"ID\": range(len(df['abstract'])),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_v')\n",
    "coherence_CV= coherence_model.get_coherence()\n",
    "print(\"C_V is: \", coherence_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "03e6425f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_mass is:  -0.5050614584843319\n"
     ]
    }
   ],
   "source": [
    "documents = pd.DataFrame({\"Document\": df['abstract'],\n",
    "                          \"ID\": range(len(df['abstract'])),\n",
    "                          \"Topic\": topics})\n",
    "\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='u_mass')\n",
    "coherence_u_mass = coherence_model.get_coherence()\n",
    "print(\"u_mass is: \",coherence_u_mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b188c5bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_npmi is:  0.08455293928381574\n"
     ]
    }
   ],
   "source": [
    "documents = pd.DataFrame({\"Document\": df['abstract'],\n",
    "                          \"ID\": range(len(df['abstract'])),\n",
    "                          \"Topic\": topics})\n",
    "\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_npmi')\n",
    "coherence_c_nmpi = coherence_model.get_coherence()\n",
    "print(\"c_npmi is: \",coherence_c_nmpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "cc49b4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9e878f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#K-mean\n",
    "from bertopic import BERTopic\n",
    "from sklearn.cluster import KMeans\n",
    "from umap import UMAP\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initiate UMAP\n",
    "umap_model = UMAP(n_neighbors=30, \n",
    "                  n_components=5, \n",
    "                  min_dist=0.0, \n",
    "                  metric='cosine', \n",
    "                  random_state=100)\n",
    "\n",
    "\n",
    "cluster_model = KMeans(n_clusters=30)\n",
    "topic_model = BERTopic(umap_model=umap_model, embedding_model=sentence_model, hdbscan_model=cluster_model)\n",
    "\n",
    "# Run BERTopic model\n",
    "topics, probabilities = topic_model.fit_transform(df['abstract']) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "10598156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_V is:  0.4438974770383022\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "documents = pd.DataFrame({\"Document\": df['abstract'],\n",
    "                          \"ID\": range(len(df['abstract'])),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_v')\n",
    "coherence_CV= coherence_model.get_coherence()\n",
    "print(\"C_V is: \", coherence_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "dd29d15a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_mass is:  -0.16209198073742098\n"
     ]
    }
   ],
   "source": [
    "documents = pd.DataFrame({\"Document\": df['abstract'],\n",
    "                          \"ID\": range(len(df['abstract'])),\n",
    "                          \"Topic\": topics})\n",
    "\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='u_mass')\n",
    "coherence_u_mass = coherence_model.get_coherence()\n",
    "print(\"u_mass is: \",coherence_u_mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "3a2617c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_npmi is:  0.03282558049581263\n"
     ]
    }
   ],
   "source": [
    "documents = pd.DataFrame({\"Document\": df['abstract'],\n",
    "                          \"ID\": range(len(df['abstract'])),\n",
    "                          \"Topic\": topics})\n",
    "\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_npmi')\n",
    "coherence_c_nmpi = coherence_model.get_coherence()\n",
    "print(\"c_npmi is: \",coherence_c_nmpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "223ebe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter Tuning for Topic Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d9b9a9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set CountVetorizer, c-TF-IDF\n",
    "from bertopic import BERTopic\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initiate UMAP\n",
    "umap_model = UMAP(n_neighbors=30, \n",
    "                  n_components=5, \n",
    "                  min_dist=0.0, \n",
    "                  metric='cosine', \n",
    "                  random_state=100)\n",
    "\n",
    "#HDBSCAN\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=30, min_samples = 10, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "\n",
    "\n",
    "#CountVectorizer\n",
    "vectorizer_model = CountVectorizer(max_features=1_000, min_df=10, ngram_range=(1, 3))\n",
    "\n",
    "#c-TF-IDF\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "\n",
    "#Run Model\n",
    "topic_model = BERTopic(umap_model=umap_model,\n",
    "                       embedding_model=sentence_model, \n",
    "                       hdbscan_model=hdbscan_model, \n",
    "                       vectorizer_model=vectorizer_model,\n",
    "                       ctfidf_model=ctfidf_model,\n",
    "                       diversity=None)\n",
    "\n",
    "# Run BERTopic model\n",
    "topics, probabilities = topic_model.fit_transform(df['abstract']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "651c08c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_V is:  0.640818345308697\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "documents = pd.DataFrame({\"Document\": df['abstract'],\n",
    "                          \"ID\": range(len(df['abstract'])),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_v')\n",
    "coherence_CV= coherence_model.get_coherence()\n",
    "print(\"C_V is: \", coherence_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ee45087b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_mass is:  -0.4149948371121392\n"
     ]
    }
   ],
   "source": [
    "documents = pd.DataFrame({\"Document\": df['abstract'],\n",
    "                          \"ID\": range(len(df['abstract'])),\n",
    "                          \"Topic\": topics})\n",
    "\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='u_mass')\n",
    "coherence_u_mass = coherence_model.get_coherence()\n",
    "print(\"u_mass is: \",coherence_u_mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "a3bebf36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_npmi is:  0.012140635891276336\n"
     ]
    }
   ],
   "source": [
    "documents = pd.DataFrame({\"Document\": df['abstract'],\n",
    "                          \"ID\": range(len(df['abstract'])),\n",
    "                          \"Topic\": topics})\n",
    "\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_npmi')\n",
    "coherence_c_nmpi = coherence_model.get_coherence()\n",
    "print(\"c_npmi is: \",coherence_c_nmpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4ee6f03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a9c3ecda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initiate UMAP\n",
    "umap_model = UMAP(n_neighbors=30, \n",
    "                  n_components=5, \n",
    "                  min_dist=0.0, \n",
    "                  metric='cosine', \n",
    "                  random_state=100)\n",
    "\n",
    "#HDBSCAN\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=30, min_samples = 10, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "\n",
    "\n",
    "#CountVectorizer\n",
    "vectorizer_model = CountVectorizer(max_features=1_000, min_df=10, ngram_range=(1, 3))\n",
    "\n",
    "#c-TF-IDF\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "\n",
    "#Run Model\n",
    "topic_model = BERTopic(umap_model=umap_model,\n",
    "                       embedding_model=sentence_model, \n",
    "                       hdbscan_model=hdbscan_model, \n",
    "                       vectorizer_model=vectorizer_model,\n",
    "                       ctfidf_model=ctfidf_model,\n",
    "                       diversity=0.8)\n",
    "\n",
    "# Run BERTopic model\n",
    "topics, probabilities = topic_model.fit_transform(df['abstract']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "a0dccab9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_V is:  0.49297913677618826\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "documents = pd.DataFrame({\"Document\": df['abstract'],\n",
    "                          \"ID\": range(len(df['abstract'])),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_v')\n",
    "coherence_CV= coherence_model.get_coherence()\n",
    "print(\"C_V is: \", coherence_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "aa1c6b02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_mass is:  -0.3957143515403035\n"
     ]
    }
   ],
   "source": [
    "documents = pd.DataFrame({\"Document\": df['abstract'],\n",
    "                          \"ID\": range(len(df['abstract'])),\n",
    "                          \"Topic\": topics})\n",
    "\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='u_mass')\n",
    "coherence_u_mass = coherence_model.get_coherence()\n",
    "print(\"u_mass is: \",coherence_u_mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ac34aee2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_npmi is:  -0.15957406872900107\n"
     ]
    }
   ],
   "source": [
    "documents = pd.DataFrame({\"Document\": df['abstract'],\n",
    "                          \"ID\": range(len(df['abstract'])),\n",
    "                          \"Topic\": topics})\n",
    "\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_npmi')\n",
    "coherence_c_nmpi = coherence_model.get_coherence()\n",
    "print(\"c_npmi is: \",coherence_c_nmpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2cdf0cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Topic Visualization without Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "78cc94bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 103/103 [00:00<00:00, 200.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "â”œâ”€data_to_information_the_for\n",
      "â”‚    â”œâ”€signal_audio_image_power_output\n",
      "â”‚    â”‚    â”œâ”€power_voltage_converter_output_circuit\n",
      "â”‚    â”‚    â”‚    â”œâ”€voltage_converter_output_signal_circuit\n",
      "â”‚    â”‚    â”‚    â”‚    â”œâ”€â– â”€â”€transistor_stage_amplifier_cascode_terminal â”€â”€ Topic: 97\n",
      "â”‚    â”‚    â”‚    â”‚    â””â”€voltage_converter_signal_circuit_power\n",
      "â”‚    â”‚    â”‚    â”‚         â”œâ”€â– â”€â”€voltage_converter_power_load_output â”€â”€ Topic: 13\n",
      "â”‚    â”‚    â”‚    â”‚         â””â”€â– â”€â”€clock_signal_circuit_frequency_comparator â”€â”€ Topic: 28\n",
      "â”‚    â”‚    â”‚    â””â”€charging_power_coil_wireless_charge\n",
      "â”‚    â”‚    â”‚         â”œâ”€â– â”€â”€coil_power_wireless_transmitter_electronic â”€â”€ Topic: 78\n",
      "â”‚    â”‚    â”‚         â””â”€â– â”€â”€charging_power_charge_adapter_supercapacitor â”€â”€ Topic: 56\n",
      "â”‚    â”‚    â””â”€audio_image_vehicle_sensor_acoustic\n",
      "â”‚    â”‚         â”œâ”€audio_vehicle_acoustic_sensor_sound\n",
      "â”‚    â”‚         â”‚    â”œâ”€audio_acoustic_sound_sensor_signal\n",
      "â”‚    â”‚         â”‚    â”‚    â”œâ”€sensor_temperature_wearable_thermoelectric_detector\n",
      "â”‚    â”‚         â”‚    â”‚    â”‚    â”œâ”€â– â”€â”€temperature_thermoelectric_thermocouple_sensor_thermal â”€â”€ Topic: 75\n",
      "â”‚    â”‚         â”‚    â”‚    â”‚    â””â”€sensor_wearable_detector_watch_operator\n",
      "â”‚    â”‚         â”‚    â”‚    â”‚         â”œâ”€â– â”€â”€sensor_detector_sensitivity_magnetic_enclosure â”€â”€ Topic: 63\n",
      "â”‚    â”‚         â”‚    â”‚    â”‚         â””â”€â– â”€â”€wearable_watch_operator_wrist_piloting â”€â”€ Topic: 60\n",
      "â”‚    â”‚         â”‚    â”‚    â””â”€audio_acoustic_sound_signal_ultrasonic\n",
      "â”‚    â”‚         â”‚    â”‚         â”œâ”€audio_sound_acoustic_signal_speaker\n",
      "â”‚    â”‚         â”‚    â”‚         â”‚    â”œâ”€â– â”€â”€stimulation_nerve_cardiac_blood_electrodes â”€â”€ Topic: 55\n",
      "â”‚    â”‚         â”‚    â”‚         â”‚    â””â”€audio_sound_acoustic_signal_speaker\n",
      "â”‚    â”‚         â”‚    â”‚         â”‚         â”œâ”€â– â”€â”€bg_medication_patient_heart_value â”€â”€ Topic: 43\n",
      "â”‚    â”‚         â”‚    â”‚         â”‚         â””â”€â– â”€â”€audio_sound_acoustic_signal_speaker â”€â”€ Topic: 6\n",
      "â”‚    â”‚         â”‚    â”‚         â””â”€â– â”€â”€ultrasonic_transducer_ultrasound_acoustic_transmitting â”€â”€ Topic: 61\n",
      "â”‚    â”‚         â”‚    â””â”€vehicle_road_speed_driving_control\n",
      "â”‚    â”‚         â”‚         â”œâ”€â– â”€â”€vehicle_road_driving_autonomous_traffic â”€â”€ Topic: 21\n",
      "â”‚    â”‚         â”‚         â””â”€â– â”€â”€vehicle_speed_torque_acceleration_engine â”€â”€ Topic: 44\n",
      "â”‚    â”‚         â””â”€image_object_3d_images_information\n",
      "â”‚    â”‚              â”œâ”€image_3d_object_images_model\n",
      "â”‚    â”‚              â”‚    â”œâ”€3d_image_object_images_model\n",
      "â”‚    â”‚              â”‚    â”‚    â”œâ”€â– â”€â”€augmented_reality_witness_display_buried â”€â”€ Topic: 85\n",
      "â”‚    â”‚              â”‚    â”‚    â””â”€3d_image_images_object_model\n",
      "â”‚    â”‚              â”‚    â”‚         â”œâ”€â– â”€â”€3d_depth_image_model_object â”€â”€ Topic: 27\n",
      "â”‚    â”‚              â”‚    â”‚         â””â”€â– â”€â”€images_image_salient_dental_recognition â”€â”€ Topic: 86\n",
      "â”‚    â”‚              â”‚    â””â”€image_unit_processing_object_images\n",
      "â”‚    â”‚              â”‚         â”œâ”€â– â”€â”€image_processing_object_boundary_unit â”€â”€ Topic: 48\n",
      "â”‚    â”‚              â”‚         â””â”€â– â”€â”€image_scan_af_scanning_unit â”€â”€ Topic: 92\n",
      "â”‚    â”‚              â””â”€information_camera_log_unit_processing\n",
      "â”‚    â”‚                   â”œâ”€â– â”€â”€camera_imager_image_photosharing_preset â”€â”€ Topic: 99\n",
      "â”‚    â”‚                   â””â”€log_information_unit_terminal_robot\n",
      "â”‚    â”‚                        â”œâ”€â– â”€â”€robot_terminal_telematics_mobile_unit â”€â”€ Topic: 94\n",
      "â”‚    â”‚                        â””â”€â– â”€â”€log_information_processing_life_unit â”€â”€ Topic: 96\n",
      "â”‚    â””â”€data_network_information_user_device\n",
      "â”‚         â”œâ”€data_network_information_wireless_user\n",
      "â”‚         â”‚    â”œâ”€memory_storage_data_logical_write\n",
      "â”‚         â”‚    â”‚    â”œâ”€â– â”€â”€memory_cell_bit_nonvolatile_cells â”€â”€ Topic: 50\n",
      "â”‚         â”‚    â”‚    â””â”€â– â”€â”€storage_memory_data_logical_write â”€â”€ Topic: 9\n",
      "â”‚         â”‚    â””â”€network_wireless_information_data_user\n",
      "â”‚         â”‚         â”œâ”€content_video_media_data_search\n",
      "â”‚         â”‚         â”‚    â”œâ”€video_media_content_picture_block\n",
      "â”‚         â”‚         â”‚    â”‚    â”œâ”€â– â”€â”€video_picture_block_encoding_transform â”€â”€ Topic: 12\n",
      "â”‚         â”‚         â”‚    â”‚    â””â”€â– â”€â”€media_content_video_presentation_multimedia â”€â”€ Topic: 29\n",
      "â”‚         â”‚         â”‚    â””â”€content_search_query_data_language\n",
      "â”‚         â”‚         â”‚         â”œâ”€â– â”€â”€search_content_text_user_document â”€â”€ Topic: 8\n",
      "â”‚         â”‚         â”‚         â””â”€â– â”€â”€data_query_metadata_database_table â”€â”€ Topic: 24\n",
      "â”‚         â”‚         â””â”€network_wireless_communication_information_access\n",
      "â”‚         â”‚              â”œâ”€network_authentication_packet_device_key\n",
      "â”‚         â”‚              â”‚    â”œâ”€â– â”€â”€authentication_key_user_account_identity â”€â”€ Topic: 20\n",
      "â”‚         â”‚              â”‚    â””â”€network_packet_message_device_node\n",
      "â”‚         â”‚              â”‚         â”œâ”€â– â”€â”€communication_conference_service_sais_address â”€â”€ Topic: 70\n",
      "â”‚         â”‚              â”‚         â””â”€â– â”€â”€network_packet_message_node_multicast â”€â”€ Topic: 10\n",
      "â”‚         â”‚              â””â”€â– â”€â”€wireless_ue_station_communication_transmission â”€â”€ Topic: 1\n",
      "â”‚         â””â”€game_payment_user_data_cloud\n",
      "â”‚              â”œâ”€game_payment_cloud_threat_user\n",
      "â”‚              â”‚    â”œâ”€cloud_threat_management_security_data\n",
      "â”‚              â”‚    â”‚    â”œâ”€â– â”€â”€energy_consumption_usage_building_utility â”€â”€ Topic: 95\n",
      "â”‚              â”‚    â”‚    â””â”€cloud_threat_security_management_network\n",
      "â”‚              â”‚    â”‚         â”œâ”€â– â”€â”€threat_security_management_endpoint_traffic â”€â”€ Topic: 57\n",
      "â”‚              â”‚    â”‚         â””â”€â– â”€â”€cloud_service_instance_application_computing â”€â”€ Topic: 38\n",
      "â”‚              â”‚    â””â”€game_payment_user_location_player\n",
      "â”‚              â”‚         â”œâ”€payment_location_user_merchant_transaction\n",
      "â”‚              â”‚         â”‚    â”œâ”€location_user_geographic_tracking_online\n",
      "â”‚              â”‚         â”‚    â”‚    â”œâ”€â– â”€â”€location_tracking_geographic_mobile_device â”€â”€ Topic: 65\n",
      "â”‚              â”‚         â”‚    â”‚    â””â”€â– â”€â”€online_ad_user_customer_sale â”€â”€ Topic: 81\n",
      "â”‚              â”‚         â”‚    â””â”€â– â”€â”€payment_merchant_card_transaction_account â”€â”€ Topic: 66\n",
      "â”‚              â”‚         â””â”€â– â”€â”€game_player_gaming_players_objects â”€â”€ Topic: 49\n",
      "â”‚              â””â”€â– â”€â”€call_caller_telephone_number_calling â”€â”€ Topic: 84\n",
      "â””â”€and_the_of_layer_first\n",
      "     â”œâ”€layer_the_first_second_and\n",
      "     â”‚    â”œâ”€the_portion_end_and_valve\n",
      "     â”‚    â”‚    â”œâ”€gas_valve_heat_fluid_air\n",
      "     â”‚    â”‚    â”‚    â”œâ”€gas_heat_turbine_air_cooling\n",
      "     â”‚    â”‚    â”‚    â”‚    â”œâ”€heat_turbine_cooling_air_compressor\n",
      "     â”‚    â”‚    â”‚    â”‚    â”‚    â”œâ”€â– â”€â”€turbine_fan_blade_compressor_engine â”€â”€ Topic: 11\n",
      "     â”‚    â”‚    â”‚    â”‚    â”‚    â””â”€â– â”€â”€heat_cooling_air_heating_exchanger â”€â”€ Topic: 5\n",
      "     â”‚    â”‚    â”‚    â”‚    â””â”€gas_fuel_exhaust_pressure_chamber\n",
      "     â”‚    â”‚    â”‚    â”‚         â”œâ”€pressure_gas_chamber_fluid_processing\n",
      "     â”‚    â”‚    â”‚    â”‚         â”‚    â”œâ”€â– â”€â”€processing_plasma_chamber_substrate_gas â”€â”€ Topic: 40\n",
      "     â”‚    â”‚    â”‚    â”‚         â”‚    â””â”€â– â”€â”€pressure_fluid_sensor_gas_flow â”€â”€ Topic: 33\n",
      "     â”‚    â”‚    â”‚    â”‚         â””â”€â– â”€â”€fuel_exhaust_gas_combustion_engine â”€â”€ Topic: 17\n",
      "     â”‚    â”‚    â”‚    â””â”€valve_water_fluid_filter_outlet\n",
      "     â”‚    â”‚    â”‚         â”œâ”€â– â”€â”€valve_fluid_pressure_inlet_piston â”€â”€ Topic: 25\n",
      "     â”‚    â”‚    â”‚         â””â”€â– â”€â”€filter_water_vacuum_air_flow â”€â”€ Topic: 47\n",
      "     â”‚    â”‚    â””â”€portion_end_member_the_and\n",
      "     â”‚    â”‚         â”œâ”€container_implant_dispensing_the_bone\n",
      "     â”‚    â”‚         â”‚    â”œâ”€implant_bone_stent_valve_distal\n",
      "     â”‚    â”‚         â”‚    â”‚    â”œâ”€â– â”€â”€stent_valve_heart_prosthetic_native â”€â”€ Topic: 68\n",
      "     â”‚    â”‚         â”‚    â”‚    â””â”€implant_bone_patient_tube_distal\n",
      "     â”‚    â”‚         â”‚    â”‚         â”œâ”€â– â”€â”€catheter_tube_distal_nasal_patient â”€â”€ Topic: 37\n",
      "     â”‚    â”‚         â”‚    â”‚         â””â”€â– â”€â”€implant_bone_anvil_knee_patient â”€â”€ Topic: 16\n",
      "     â”‚    â”‚         â”‚    â””â”€container_dispensing_cartridge_conveyor_beverage\n",
      "     â”‚    â”‚         â”‚         â”œâ”€container_dispensing_cartridge_conveyor_beverage\n",
      "     â”‚    â”‚         â”‚         â”‚    â”œâ”€conveyor_toner_transfer_forming_image\n",
      "     â”‚    â”‚         â”‚         â”‚    â”‚    â”œâ”€â– â”€â”€toner_forming_image_developing_apparatus â”€â”€ Topic: 64\n",
      "     â”‚    â”‚         â”‚         â”‚    â”‚    â””â”€â– â”€â”€conveyor_transfer_roller_transport_freight â”€â”€ Topic: 46\n",
      "     â”‚    â”‚         â”‚         â”‚    â””â”€container_dispensing_beverage_cartridge_cap\n",
      "     â”‚    â”‚         â”‚         â”‚         â”œâ”€â– â”€â”€container_portion_shipping_flange_wall â”€â”€ Topic: 41\n",
      "     â”‚    â”‚         â”‚         â”‚         â””â”€â– â”€â”€dispensing_beverage_cartridge_container_dispenser â”€â”€ Topic: 14\n",
      "     â”‚    â”‚         â”‚         â””â”€â– â”€â”€lock_locking_door_locks_unlocking â”€â”€ Topic: 93\n",
      "     â”‚    â”‚         â””â”€portion_antenna_end_member_second\n",
      "     â”‚    â”‚              â”œâ”€ring_bearing_seal_outer_inner\n",
      "     â”‚    â”‚              â”‚    â”œâ”€ring_bearing_seal_outer_shaft\n",
      "     â”‚    â”‚              â”‚    â”‚    â”œâ”€â– â”€â”€seal_pipe_tube_hose_sealing â”€â”€ Topic: 42\n",
      "     â”‚    â”‚              â”‚    â”‚    â””â”€â– â”€â”€ring_bearing_shaft_outer_inner â”€â”€ Topic: 36\n",
      "     â”‚    â”‚              â”‚    â””â”€actuator_damper_damping_member_vibration\n",
      "     â”‚    â”‚              â”‚         â”œâ”€â– â”€â”€damper_damping_spring_vibration_shockabsorbing â”€â”€ Topic: 88\n",
      "     â”‚    â”‚              â”‚         â””â”€â– â”€â”€actuator_slide_member_direction_snare â”€â”€ Topic: 90\n",
      "     â”‚    â”‚              â””â”€antenna_portion_seat_side_end\n",
      "     â”‚    â”‚                   â”œâ”€seat_portion_connector_member_end\n",
      "     â”‚    â”‚                   â”‚    â”œâ”€â– â”€â”€garment_partition_rear_walls_portion â”€â”€ Topic: 51\n",
      "     â”‚    â”‚                   â”‚    â””â”€seat_connector_portion_member_end\n",
      "     â”‚    â”‚                   â”‚         â”œâ”€â– â”€â”€seat_support_member_portion_frame â”€â”€ Topic: 4\n",
      "     â”‚    â”‚                   â”‚         â””â”€â– â”€â”€connector_cable_electrical_contact_housing â”€â”€ Topic: 23\n",
      "     â”‚    â”‚                   â””â”€antenna_radiator_solar_conductor_rf\n",
      "     â”‚    â”‚                        â”œâ”€â– â”€â”€antenna_radiator_conductor_rf_feed â”€â”€ Topic: 19\n",
      "     â”‚    â”‚                        â””â”€â– â”€â”€solar_photovoltaic_panel_pv_electrode â”€â”€ Topic: 35\n",
      "     â”‚    â””â”€layer_substrate_semiconductor_light_display\n",
      "     â”‚         â”œâ”€layer_semiconductor_substrate_gate_display\n",
      "     â”‚         â”‚    â”œâ”€display_touch_panel_substrate_pixel\n",
      "     â”‚         â”‚    â”‚    â”œâ”€â– â”€â”€crystal_liquid_display_transparent_panel â”€â”€ Topic: 67\n",
      "     â”‚         â”‚    â”‚    â””â”€display_touch_substrate_panel_pixel\n",
      "     â”‚         â”‚    â”‚         â”œâ”€â– â”€â”€display_panel_pixel_substrate_lines â”€â”€ Topic: 7\n",
      "     â”‚         â”‚    â”‚         â””â”€â– â”€â”€touch_electrode_screen_electrodes_sensing â”€â”€ Topic: 22\n",
      "     â”‚         â”‚    â””â”€semiconductor_layer_gate_substrate_region\n",
      "     â”‚         â”‚         â”œâ”€â– â”€â”€semiconductor_layer_gate_substrate_conductive â”€â”€ Topic: 2\n",
      "     â”‚         â”‚         â””â”€â– â”€â”€memory_mram_region_layer_layers â”€â”€ Topic: 32\n",
      "     â”‚         â””â”€light_emitting_lightemitting_organic_layer\n",
      "     â”‚              â”œâ”€light_source_lighting_plate_led\n",
      "     â”‚              â”‚    â”œâ”€â– â”€â”€photoelectric_light_conversion_sensor_image â”€â”€ Topic: 45\n",
      "     â”‚              â”‚    â””â”€light_source_lighting_led_plate\n",
      "     â”‚              â”‚         â”œâ”€â– â”€â”€light_lighting_led_source_lamp â”€â”€ Topic: 15\n",
      "     â”‚              â”‚         â””â”€â– â”€â”€light_plate_guide_display_backlight â”€â”€ Topic: 59\n",
      "     â”‚              â””â”€organic_emitting_lightemitting_light_layer\n",
      "     â”‚                   â”œâ”€â– â”€â”€emitting_lightemitting_light_led_layer â”€â”€ Topic: 69\n",
      "     â”‚                   â””â”€â– â”€â”€organic_pixel_emitting_el_light â”€â”€ Topic: 79\n",
      "     â””â”€invention_and_or_of_optical\n",
      "          â”œâ”€and_or_of_optical_invention\n",
      "          â”‚    â”œâ”€optical_lens_laser_the_an\n",
      "          â”‚    â”‚    â”œâ”€gear_cutting_magnetic_rotor_brake\n",
      "          â”‚    â”‚    â”‚    â”œâ”€magnetic_resonance_pulse_gradient_readout\n",
      "          â”‚    â”‚    â”‚    â”‚    â”œâ”€â– â”€â”€resonance_magnetic_pulse_gradient_readout â”€â”€ Topic: 83\n",
      "          â”‚    â”‚    â”‚    â”‚    â””â”€â– â”€â”€magnetic_suspension_magnetization_pole_piece â”€â”€ Topic: 89\n",
      "          â”‚    â”‚    â”‚    â””â”€gear_cutting_rotor_brake_shaft\n",
      "          â”‚    â”‚    â”‚         â”œâ”€gear_rotor_brake_clutch_planetary\n",
      "          â”‚    â”‚    â”‚         â”‚    â”œâ”€gear_rotor_planetary_clutch_stator\n",
      "          â”‚    â”‚    â”‚         â”‚    â”‚    â”œâ”€â– â”€â”€rotor_stator_core_motor_proprotor â”€â”€ Topic: 71\n",
      "          â”‚    â”‚    â”‚         â”‚    â”‚    â””â”€â– â”€â”€gear_planetary_clutch_shaft_drive â”€â”€ Topic: 39\n",
      "          â”‚    â”‚    â”‚         â”‚    â””â”€brake_hydraulic_wheel_agricultural_crop\n",
      "          â”‚    â”‚    â”‚         â”‚         â”œâ”€â– â”€â”€brake_hydraulic_wheel_braking_pressure â”€â”€ Topic: 82\n",
      "          â”‚    â”‚    â”‚         â”‚         â””â”€â– â”€â”€agricultural_crop_weeding_seed_row â”€â”€ Topic: 91\n",
      "          â”‚    â”‚    â”‚         â””â”€cutting_xray_golf_sole_handle\n",
      "          â”‚    â”‚    â”‚              â”œâ”€cutting_golf_sole_tool_shoe\n",
      "          â”‚    â”‚    â”‚              â”‚    â”œâ”€cutting_golf_tool_swing_handle\n",
      "          â”‚    â”‚    â”‚              â”‚    â”‚    â”œâ”€â– â”€â”€golf_swing_ball_club_bat â”€â”€ Topic: 54\n",
      "          â”‚    â”‚    â”‚              â”‚    â”‚    â””â”€cutting_tool_handle_blade_clamping\n",
      "          â”‚    â”‚    â”‚              â”‚    â”‚         â”œâ”€â– â”€â”€exercise_resistance_pulley_ergonomic_apparatus â”€â”€ Topic: 103\n",
      "          â”‚    â”‚    â”‚              â”‚    â”‚         â””â”€cutting_tool_blade_clamping_handle\n",
      "          â”‚    â”‚    â”‚              â”‚    â”‚              â”œâ”€â– â”€â”€clamping_handle_grip_sheath_holding â”€â”€ Topic: 80\n",
      "          â”‚    â”‚    â”‚              â”‚    â”‚              â””â”€â– â”€â”€cutting_tool_blade_pipe_blades â”€â”€ Topic: 58\n",
      "          â”‚    â”‚    â”‚              â”‚    â””â”€â– â”€â”€sole_shoe_sock_ski_heel â”€â”€ Topic: 87\n",
      "          â”‚    â”‚    â”‚              â””â”€xray_bolt_xrays_gun_firearm\n",
      "          â”‚    â”‚    â”‚                   â”œâ”€â– â”€â”€bolt_gun_firearm_trigger_lug â”€â”€ Topic: 102\n",
      "          â”‚    â”‚    â”‚                   â””â”€â– â”€â”€xray_xrays_tube_detector_modification â”€â”€ Topic: 98\n",
      "          â”‚    â”‚    â””â”€optical_lens_laser_ink_print\n",
      "          â”‚    â”‚         â”œâ”€optical_lens_laser_refractive_beam\n",
      "          â”‚    â”‚         â”‚    â”œâ”€optical_lens_refractive_photonic_waveguide\n",
      "          â”‚    â”‚         â”‚    â”‚    â”œâ”€â– â”€â”€lens_optical_refractive_imageside_concave â”€â”€ Topic: 31\n",
      "          â”‚    â”‚         â”‚    â”‚    â””â”€â– â”€â”€optical_photonic_waveguide_fiber_refractive â”€â”€ Topic: 18\n",
      "          â”‚    â”‚         â”‚    â””â”€laser_beam_irradiation_radiation_particle\n",
      "          â”‚    â”‚         â”‚         â”œâ”€â– â”€â”€laser_beam_workpiece_treatment_energy â”€â”€ Topic: 73\n",
      "          â”‚    â”‚         â”‚         â””â”€â– â”€â”€beam_radiation_irradiation_particle_dose â”€â”€ Topic: 72\n",
      "          â”‚    â”‚         â””â”€ink_print_printing_printer_head\n",
      "          â”‚    â”‚              â”œâ”€â– â”€â”€print_printing_printer_setting_page â”€â”€ Topic: 76\n",
      "          â”‚    â”‚              â””â”€â– â”€â”€ink_printing_head_print_nozzle â”€â”€ Topic: 34\n",
      "          â”‚    â””â”€invention_composition_acid_compositions_or\n",
      "          â”‚         â”œâ”€invention_acid_composition_compositions_or\n",
      "          â”‚         â”‚    â”œâ”€invention_acid_compositions_composition_or\n",
      "          â”‚         â”‚    â”‚    â”œâ”€acid_invention_compositions_composition_methods\n",
      "          â”‚         â”‚    â”‚    â”‚    â”œâ”€â– â”€â”€compositions_invention_acid_methods_nucleic â”€â”€ Topic: 0\n",
      "          â”‚         â”‚    â”‚    â”‚    â””â”€â– â”€â”€composition_catalyst_polymer_comprising_step â”€â”€ Topic: 3\n",
      "          â”‚         â”‚    â”‚    â””â”€alloy_layer_aluminum_weight_coating\n",
      "          â”‚         â”‚    â”‚         â”œâ”€â– â”€â”€alloy_layer_aluminum_glass_weight â”€â”€ Topic: 30\n",
      "          â”‚         â”‚    â”‚         â””â”€â– â”€â”€part_material_additive_manufacturing_formed â”€â”€ Topic: 62\n",
      "          â”‚         â”‚    â””â”€battery_electrolyte_electrode_lithium_anode\n",
      "          â”‚         â”‚         â”œâ”€â– â”€â”€electrolyte_battery_electrode_lithium_anode â”€â”€ Topic: 26\n",
      "          â”‚         â”‚         â””â”€â– â”€â”€battery_electrode_case_terminal_connecting â”€â”€ Topic: 74\n",
      "          â”‚         â””â”€tire_rubber_composite_carbon_tread\n",
      "          â”‚              â”œâ”€â– â”€â”€graphene_boron_nitride_nanotubes_borane â”€â”€ Topic: 100\n",
      "          â”‚              â””â”€tire_rubber_composite_tread_carbon\n",
      "          â”‚                   â”œâ”€â– â”€â”€composite_constituent_ply_fibers_ceramic â”€â”€ Topic: 101\n",
      "          â”‚                   â””â”€â– â”€â”€tire_rubber_tread_carbon_tirederived â”€â”€ Topic: 77\n",
      "          â””â”€soybean_cultivar_plant_plants_variety\n",
      "               â”œâ”€â– â”€â”€iplant_habit_new_characterized_named â”€â”€ Topic: 52\n",
      "               â””â”€â– â”€â”€soybean_cultivar_plant_plants_parts â”€â”€ Topic: 53\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Default BERTopic\n",
    "from bertopic import BERTopic\n",
    "\n",
    "\n",
    "topic_model = BERTopic()\n",
    "\n",
    "# Run BERTopic model\n",
    "topics, probabilities = topic_model.fit_transform(df['abstract']) \n",
    "hierarchical_topics = topic_model.hierarchical_topics(df['abstract'])\n",
    "tree = topic_model.get_topic_tree(hierarchical_topics)\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85230e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
