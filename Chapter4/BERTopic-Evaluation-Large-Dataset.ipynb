{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1294f85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The subject matters of the invention are: a cr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The present invention relates to the treatment...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The invention relates to a composition compris...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The invention relates to an improved process f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The present invention relates to a new method ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93657</th>\n",
       "      <td>Individual measuring sensors each have a diffe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93658</th>\n",
       "      <td>Vehicle (10, 100) comprising at least three dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93659</th>\n",
       "      <td>A customer loyalty programme operated by an op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93660</th>\n",
       "      <td>An inflatable pipe tube fabric assembly for G-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93661</th>\n",
       "      <td>A deck leverage anchor (40) for securing an ex...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>93662 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                abstract\n",
       "0      The subject matters of the invention are: a cr...\n",
       "1      The present invention relates to the treatment...\n",
       "2      The invention relates to a composition compris...\n",
       "3      The invention relates to an improved process f...\n",
       "4      The present invention relates to a new method ...\n",
       "...                                                  ...\n",
       "93657  Individual measuring sensors each have a diffe...\n",
       "93658  Vehicle (10, 100) comprising at least three dr...\n",
       "93659  A customer loyalty programme operated by an op...\n",
       "93660  An inflatable pipe tube fabric assembly for G-...\n",
       "93661  A deck leverage anchor (40) for securing an ex...\n",
       "\n",
       "[93662 rows x 1 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "df = pd.read_csv('pppm_abstract.csv')\n",
    "df=df.dropna()\n",
    "df=df.reset_index(drop=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffb73804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default BERTopic\n",
    "from bertopic import BERTopic\n",
    "\n",
    "\n",
    "topic_model = BERTopic()\n",
    "\n",
    "# Run BERTopic model\n",
    "topics, probabilities = topic_model.fit_transform(df['abstract']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58bc5272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C_V is:  0.6136640457155098\n"
     ]
    }
   ],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "documents = pd.DataFrame({\"Document\": df['abstract'],\n",
    "                          \"ID\": range(len(df['abstract'])),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_v')\n",
    "coherence_CV= coherence_model.get_coherence()\n",
    "print(\"C_V is: \", coherence_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f8d53d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "u_mass is:  -1.4340323118745282\n"
     ]
    }
   ],
   "source": [
    "documents = pd.DataFrame({\"Document\": df['abstract'],\n",
    "                          \"ID\": range(len(df['abstract'])),\n",
    "                          \"Topic\": topics})\n",
    "\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='u_mass')\n",
    "coherence_u_mass = coherence_model.get_coherence()\n",
    "print(\"u_mass is: \",coherence_u_mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19e78b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c_npmi is:  0.07446102591447082\n"
     ]
    }
   ],
   "source": [
    "documents = pd.DataFrame({\"Document\": df['abstract'],\n",
    "                          \"ID\": range(len(df['abstract'])),\n",
    "                          \"Topic\": topics})\n",
    "\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_npmi')\n",
    "coherence_c_nmpi = coherence_model.get_coherence()\n",
    "print(\"c_npmi is: \",coherence_c_nmpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6e53a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BERTopic With Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0168ecd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn==1.2.0\n",
      "  Using cached scikit_learn-1.2.0-cp39-cp39-win_amd64.whl (8.3 MB)\n",
      "Collecting numpy>=1.17.3\n",
      "  Using cached numpy-1.24.1-cp39-cp39-win_amd64.whl (14.9 MB)\n",
      "Collecting joblib>=1.1.1\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting scipy>=1.3.2\n",
      "  Using cached scipy-1.10.0-cp39-cp39-win_amd64.whl (42.5 MB)\n",
      "Installing collected packages: threadpoolctl, numpy, joblib, scipy, scikit-learn\n",
      "Successfully installed joblib-1.2.0 numpy-1.24.1 scikit-learn-1.2.0 scipy-1.10.0 threadpoolctl-3.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script f2py.exe is installed in 'C:\\Users\\admin\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "daal4py 2021.3.0 requires daal==2021.2.3, which is not installed.\n",
      "pandas-profiling 3.3.0 requires joblib~=1.1.0, but you have joblib 1.2.0 which is incompatible.\n",
      "pandas-profiling 3.3.0 requires numpy<1.24,>=1.16.0, but you have numpy 1.24.1 which is incompatible.\n",
      "pandas-profiling 3.3.0 requires scipy<1.10,>=1.4.1, but you have scipy 1.10.0 which is incompatible.\n",
      "octis 1.10.4 requires scikit-learn==0.24.2, but you have scikit-learn 1.2.0 which is incompatible.\n",
      "numba 0.54.1 requires numpy<1.21,>=1.17, but you have numpy 1.24.1 which is incompatible.\n",
      "moviepy 1.0.3 requires decorator<5.0,>=4.0.2, but you have decorator 5.1.1 which is incompatible.\n",
      "ale-py 0.8.0 requires importlib-metadata>=4.10.0; python_version < \"3.10\", but you have importlib-metadata 4.8.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade --force-reinstall --user scikit-learn==1.2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7e79ef30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (1.2.0)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.2.1-cp39-cp39-win_amd64.whl (8.4 MB)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.24.1)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.10.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (3.1.0)\n",
      "Installing collected packages: scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.2.0\n",
      "    Uninstalling scikit-learn-1.2.0:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "octis 1.10.4 requires scikit-learn==0.24.2, but you have scikit-learn 1.2.1 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Successfully uninstalled scikit-learn-1.2.0\n",
      "Successfully installed scikit-learn-1.2.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6527a7a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bertopic==0.12.0\n",
      "  Using cached bertopic-0.12.0-py2.py3-none-any.whl (90 kB)\n",
      "Collecting hdbscan>=0.8.28\n",
      "  Using cached hdbscan-0.8.29-cp39-cp39-win_amd64.whl\n",
      "Collecting tqdm>=4.41.1\n",
      "  Using cached tqdm-4.64.1-py2.py3-none-any.whl (78 kB)\n",
      "Collecting pyyaml<6.0\n",
      "  Using cached PyYAML-5.4.1-cp39-cp39-win_amd64.whl (213 kB)\n",
      "Collecting scikit-learn>=0.22.2.post1\n",
      "  Using cached scikit_learn-1.2.1-cp39-cp39-win_amd64.whl (8.4 MB)\n",
      "Collecting umap-learn>=0.5.0\n",
      "  Using cached umap_learn-0.5.3-py3-none-any.whl\n",
      "Collecting plotly>=4.7.0\n",
      "  Downloading plotly-5.13.0-py2.py3-none-any.whl (15.2 MB)\n",
      "     ---------------------------------------- 15.2/15.2 MB 2.6 MB/s eta 0:00:00\n",
      "Collecting pandas>=1.1.5\n",
      "  Downloading pandas-1.5.3-cp39-cp39-win_amd64.whl (10.9 MB)\n",
      "     ---------------------------------------- 10.9/10.9 MB 3.3 MB/s eta 0:00:00\n",
      "Collecting numpy>=1.20.0\n",
      "  Using cached numpy-1.24.1-cp39-cp39-win_amd64.whl (14.9 MB)\n",
      "Collecting sentence-transformers>=0.4.1\n",
      "  Using cached sentence_transformers-2.2.2-py3-none-any.whl\n",
      "Collecting joblib>=1.0\n",
      "  Using cached joblib-1.2.0-py3-none-any.whl (297 kB)\n",
      "Collecting cython>=0.27\n",
      "  Downloading Cython-0.29.33-py2.py3-none-any.whl (987 kB)\n",
      "     -------------------------------------- 987.3/987.3 kB 3.3 MB/s eta 0:00:00\n",
      "Collecting scipy>=1.0\n",
      "  Using cached scipy-1.10.0-cp39-cp39-win_amd64.whl (42.5 MB)\n",
      "Collecting python-dateutil>=2.8.1\n",
      "  Using cached python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
      "Collecting pytz>=2020.1\n",
      "  Downloading pytz-2022.7.1-py2.py3-none-any.whl (499 kB)\n",
      "     -------------------------------------- 499.4/499.4 kB 3.5 MB/s eta 0:00:00\n",
      "Collecting tenacity>=6.2.0\n",
      "  Using cached tenacity-8.1.0-py3-none-any.whl (23 kB)\n",
      "Collecting threadpoolctl>=2.0.0\n",
      "  Using cached threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
      "Collecting huggingface-hub>=0.4.0\n",
      "  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n",
      "     -------------------------------------- 190.3/190.3 kB 3.8 MB/s eta 0:00:00\n",
      "Collecting sentencepiece\n",
      "  Using cached sentencepiece-0.1.97-cp39-cp39-win_amd64.whl (1.1 MB)\n",
      "Collecting torch>=1.6.0\n",
      "  Downloading torch-1.13.1-cp39-cp39-win_amd64.whl (162.5 MB)\n",
      "     -------------------------------------- 162.5/162.5 MB 2.7 MB/s eta 0:00:00\n",
      "Collecting torchvision\n",
      "  Downloading torchvision-0.14.1-cp39-cp39-win_amd64.whl (1.1 MB)\n",
      "     ---------------------------------------- 1.1/1.1 MB 3.2 MB/s eta 0:00:00\n",
      "Collecting nltk\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "     ---------------------------------------- 1.5/1.5 MB 2.5 MB/s eta 0:00:00\n",
      "Collecting transformers<5.0.0,>=4.6.0\n",
      "  Downloading transformers-4.26.0-py3-none-any.whl (6.3 MB)\n",
      "     ---------------------------------------- 6.3/6.3 MB 2.5 MB/s eta 0:00:00\n",
      "Collecting colorama\n",
      "  Using cached colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
      "Collecting pynndescent>=0.5\n",
      "  Using cached pynndescent-0.5.8-py3-none-any.whl\n",
      "Collecting numba>=0.49\n",
      "  Using cached numba-0.56.4-cp39-cp39-win_amd64.whl (2.5 MB)\n",
      "Collecting typing-extensions>=3.7.4.3\n",
      "  Using cached typing_extensions-4.4.0-py3-none-any.whl (26 kB)\n",
      "Collecting requests\n",
      "  Downloading requests-2.28.2-py3-none-any.whl (62 kB)\n",
      "     ---------------------------------------- 62.8/62.8 kB 3.5 MB/s eta 0:00:00\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.9.0-py3-none-any.whl (9.7 kB)\n",
      "Collecting packaging>=20.9\n",
      "  Downloading packaging-23.0-py3-none-any.whl (42 kB)\n",
      "     ---------------------------------------- 42.7/42.7 kB ? eta 0:00:00\n",
      "Collecting llvmlite<0.40,>=0.39.0dev0\n",
      "  Using cached llvmlite-0.39.1-cp39-cp39-win_amd64.whl (23.2 MB)\n",
      "Collecting numpy>=1.20.0\n",
      "  Using cached numpy-1.23.5-cp39-cp39-win_amd64.whl (14.7 MB)\n",
      "Collecting setuptools\n",
      "  Downloading setuptools-66.1.1-py3-none-any.whl (1.3 MB)\n",
      "     ---------------------------------------- 1.3/1.3 MB 3.2 MB/s eta 0:00:00\n",
      "Collecting six>=1.5\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Using cached tokenizers-0.13.2-cp39-cp39-win_amd64.whl (3.3 MB)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2022.10.31-cp39-cp39-win_amd64.whl (267 kB)\n",
      "     -------------------------------------- 267.8/267.8 kB 5.5 MB/s eta 0:00:00\n",
      "Collecting click\n",
      "  Using cached click-8.1.3-py3-none-any.whl (96 kB)\n",
      "Collecting pillow!=8.3.*,>=5.3.0\n",
      "  Downloading Pillow-9.4.0-cp39-cp39-win_amd64.whl (2.5 MB)\n",
      "     ---------------------------------------- 2.5/2.5 MB 3.3 MB/s eta 0:00:00\n",
      "Collecting charset-normalizer<4,>=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script normalizer.exe is installed in 'C:\\Users\\admin\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script f2py.exe is installed in 'C:\\Users\\admin\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts cygdb.exe, cython.exe and cythonize.exe are installed in 'C:\\Users\\admin\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script tqdm.exe is installed in 'C:\\Users\\admin\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts convert-caffe2-to-onnx.exe, convert-onnx-to-caffe2.exe and torchrun.exe are installed in 'C:\\Users\\admin\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script nltk.exe is installed in 'C:\\Users\\admin\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script huggingface-cli.exe is installed in 'C:\\Users\\admin\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script transformers-cli.exe is installed in 'C:\\Users\\admin\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "spyder 5.1.5 requires numpydoc>=0.6.0, which is not installed.\n",
      "spyder 5.1.5 requires pyqt5<5.13, which is not installed.\n",
      "spyder 5.1.5 requires pyqtwebengine<5.13, which is not installed.\n",
      "daal4py 2021.3.0 requires daal==2021.2.3, which is not installed.\n",
      "conda-repo-cli 1.0.4 requires pathlib, which is not installed.\n",
      "anaconda-project 0.10.1 requires ruamel-yaml, which is not installed.\n",
      "spacy-transformers 1.2.0 requires transformers<4.26.0,>=3.4.0, but you have transformers 4.26.0 which is incompatible.\n",
      "refinitiv-data 1.0.0b9 requires pandas<1.4.0,>=1.3.5, but you have pandas 1.5.3 which is incompatible.\n",
      "refinitiv-data 1.0.0b9 requires tenacity<8.1,>=8.0, but you have tenacity 8.1.0 which is incompatible.\n",
      "pandas-profiling 3.3.0 requires joblib~=1.1.0, but you have joblib 1.2.0 which is incompatible.\n",
      "pandas-profiling 3.3.0 requires pandas!=1.4.0,<1.5,>1.1, but you have pandas 1.5.3 which is incompatible.\n",
      "pandas-profiling 3.3.0 requires scipy<1.10,>=1.4.1, but you have scipy 1.10.0 which is incompatible.\n",
      "octis 1.10.4 requires scikit-learn==0.24.2, but you have scikit-learn 1.2.1 which is incompatible.\n",
      "moviepy 1.0.3 requires decorator<5.0,>=4.0.2, but you have decorator 5.1.1 which is incompatible.\n",
      "konoha 4.6.5 requires importlib-metadata<4.0.0,>=3.7.0, but you have importlib-metadata 4.8.1 which is incompatible.\n",
      "gensim 4.2.0 requires Cython==0.29.28, but you have cython 0.29.33 which is incompatible.\n",
      "flair 0.11.3 requires sentencepiece==0.1.95, but you have sentencepiece 0.1.97 which is incompatible.\n",
      "astroid 2.6.6 requires wrapt<1.13,>=1.11, but you have wrapt 1.14.0 which is incompatible.\n",
      "ale-py 0.8.0 requires importlib-metadata>=4.10.0; python_version < \"3.10\", but you have importlib-metadata 4.8.1 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading charset_normalizer-3.0.1-cp39-cp39-win_amd64.whl (96 kB)\n",
      "     ---------------------------------------- 96.5/96.5 kB 2.8 MB/s eta 0:00:00\n",
      "Collecting urllib3<1.27,>=1.21.1\n",
      "  Downloading urllib3-1.26.14-py2.py3-none-any.whl (140 kB)\n",
      "     -------------------------------------- 140.6/140.6 kB 4.2 MB/s eta 0:00:00\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2022.12.7-py3-none-any.whl (155 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Installing collected packages: tokenizers, sentencepiece, pytz, charset-normalizer, urllib3, typing-extensions, threadpoolctl, tenacity, six, setuptools, regex, pyyaml, pillow, packaging, numpy, llvmlite, joblib, idna, filelock, cython, colorama, certifi, tqdm, torch, scipy, requests, python-dateutil, plotly, numba, click, torchvision, scikit-learn, pandas, nltk, huggingface-hub, transformers, pynndescent, hdbscan, umap-learn, sentence-transformers, bertopic\n",
      "  Attempting uninstall: threadpoolctl\n",
      "    Found existing installation: threadpoolctl 3.1.0\n",
      "    Uninstalling threadpoolctl-3.1.0:\n",
      "      Successfully uninstalled threadpoolctl-3.1.0\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.2.0\n",
      "    Uninstalling joblib-1.2.0:\n",
      "      Successfully uninstalled joblib-1.2.0\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.10.0\n",
      "    Uninstalling scipy-1.10.0:\n",
      "      Successfully uninstalled scipy-1.10.0\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.2.0\n",
      "    Uninstalling scikit-learn-1.2.0:\n",
      "      Successfully uninstalled scikit-learn-1.2.0\n",
      "Successfully installed bertopic-0.12.0 certifi-2022.12.7 charset-normalizer-3.0.1 click-8.1.3 colorama-0.4.6 cython-0.29.33 filelock-3.9.0 hdbscan-0.8.29 huggingface-hub-0.12.0 idna-3.4 joblib-1.2.0 llvmlite-0.39.1 nltk-3.8.1 numba-0.56.4 numpy-1.23.5 packaging-23.0 pandas-1.5.3 pillow-9.4.0 plotly-5.13.0 pynndescent-0.5.8 python-dateutil-2.8.2 pytz-2022.7.1 pyyaml-5.4.1 regex-2022.10.31 requests-2.28.2 scikit-learn-1.2.1 scipy-1.10.0 sentence-transformers-2.2.2 sentencepiece-0.1.97 setuptools-66.1.1 six-1.16.0 tenacity-8.1.0 threadpoolctl-3.1.0 tokenizers-0.13.2 torch-1.13.1 torchvision-0.14.1 tqdm-4.64.1 transformers-4.26.0 typing-extensions-4.4.0 umap-learn-0.5.3 urllib3-1.26.14\n"
     ]
    }
   ],
   "source": [
    "!pip install --force-reinstall --user bertopic==0.12.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6dda0e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: h5py in c:\\programdata\\anaconda3\\lib\\site-packages (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.14.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from h5py) (1.24.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1cd3d79d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: typing-extensions in c:\\programdata\\anaconda3\\lib\\site-packages (4.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install typing-extensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4805c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wheel in c:\\programdata\\anaconda3\\lib\\site-packages (0.38.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cc4ee22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The scikit-learn version is 1.2.1.\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print('The scikit-learn version is {}.'.format(sklearn.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c72ec331",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\programdata\\anaconda3\\lib\\site-packages (22.3.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99fef903",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (1.24.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b24db0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic import BERTopic\n",
    "from hdbscan import HDBSCAN\n",
    "from umap import UMAP\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Initiate UMAP\n",
    "umap_model = UMAP(n_neighbors=30, \n",
    "                  n_components=5, \n",
    "                  min_dist=0.0, \n",
    "                  metric='cosine', \n",
    "                  random_state=100)\n",
    "\n",
    "#HDBSCAN\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=30, min_samples = 10, metric='euclidean', cluster_selection_method='eom', prediction_data=True)\n",
    "\n",
    "\n",
    "#CountVectorizer\n",
    "vectorizer_model = CountVectorizer(max_features=1_000, min_df=10, ngram_range=(1, 3))\n",
    "\n",
    "#c-TF-IDF\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "\n",
    "#Run Model\n",
    "topic_model = BERTopic(umap_model=umap_model,\n",
    "                       embedding_model=sentence_model, \n",
    "                       hdbscan_model=hdbscan_model, \n",
    "                       vectorizer_model=vectorizer_model,\n",
    "                       ctfidf_model=ctfidf_model,\n",
    "                       diversity=None)\n",
    "\n",
    "# Run BERTopic model\n",
    "topics, probabilities = topic_model.fit_transform(df['abstract']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bcc595",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.corpora as corpora\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "documents = pd.DataFrame({\"Document\": df['abstract'],\n",
    "                          \"ID\": range(len(df['abstract'])),\n",
    "                          \"Topic\": topics})\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_v')\n",
    "coherence_CV= coherence_model.get_coherence()\n",
    "print(\"C_V is: \", coherence_CV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7fbeaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = pd.DataFrame({\"Document\": df['abstract'],\n",
    "                          \"ID\": range(len(df['abstract'])),\n",
    "                          \"Topic\": topics})\n",
    "\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='u_mass')\n",
    "coherence_u_mass = coherence_model.get_coherence()\n",
    "print(\"u_mass is: \",coherence_u_mass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba24217e",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = pd.DataFrame({\"Document\": df['abstract'],\n",
    "                          \"ID\": range(len(df['abstract'])),\n",
    "                          \"Topic\": topics})\n",
    "\n",
    "documents_per_topic = documents.groupby(['Topic'], as_index=False).agg({'Document': ' '.join})\n",
    "cleaned_docs = topic_model._preprocess_text(documents_per_topic.Document.values)\n",
    "\n",
    "# Extract vectorizer and analyzer from BERTopic\n",
    "vectorizer = topic_model.vectorizer_model\n",
    "analyzer = vectorizer.build_analyzer()\n",
    "\n",
    "# Extract features for Topic Coherence evaluation\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tokens = [analyzer(doc) for doc in cleaned_docs]\n",
    "dictionary = corpora.Dictionary(tokens)\n",
    "corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "topic_words = [[words for words, _ in topic_model.get_topic(topic)] \n",
    "               for topic in range(len(set(topics))-1)]\n",
    "\n",
    "# Evaluate\n",
    "coherence_model = CoherenceModel(topics=topic_words, \n",
    "                                 texts=tokens, \n",
    "                                 corpus=corpus,\n",
    "                                 dictionary=dictionary, \n",
    "                                 coherence='c_npmi')\n",
    "coherence_c_nmpi = coherence_model.get_coherence()\n",
    "print(\"c_npmi is: \",coherence_c_nmpi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
